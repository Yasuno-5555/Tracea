//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36836380
// Cuda compilation tools, release 13.1, V13.1.80
// Based on NVVM 7.0.1
//

.version 9.1
.target sm_80
.address_size 64

	// .globl	conv2d_implicit_gemm
.extern .shared .align 16 .b8 smem[];

.visible .entry conv2d_implicit_gemm(
	.param .u64 conv2d_implicit_gemm_param_0,
	.param .u64 conv2d_implicit_gemm_param_1,
	.param .u64 conv2d_implicit_gemm_param_2
)
.maxntid 128, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<109>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<720>;
	.reg .b64 	%rd<199>;


	ld.param.u64 	%rd10, [conv2d_implicit_gemm_param_0];
	ld.param.u64 	%rd11, [conv2d_implicit_gemm_param_1];
	ld.param.u64 	%rd12, [conv2d_implicit_gemm_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r176, %ctaid.x;
	mul.wide.u32 	%rd1, %r176, 101;
	mov.u32 	%r177, %nctaid.x;
	cvt.u64.u32 	%rd2, %r177;
	and.b64  	%rd13, %rd1, 545460846592;
	setp.eq.s64 	%p1, %rd13, 0;
	@%p1 bra 	$L__BB0_2;

	rem.u64 	%rd198, %rd1, %rd2;
	bra.uni 	$L__BB0_3;

$L__BB0_2:
	cvt.u32.u64 	%r178, %rd2;
	cvt.u32.u64 	%r179, %rd1;
	rem.u32 	%r180, %r179, %r178;
	cvt.u64.u32 	%rd198, %r180;

$L__BB0_3:
	cvt.u32.u64 	%r181, %rd198;
	shl.b32 	%r2, %r181, 6;
	mov.u32 	%r182, %ctaid.y;
	shl.b32 	%r3, %r182, 6;
	setp.gt.s32 	%p2, %r1, 4095;
	@%p2 bra 	$L__BB0_10;

	max.s32 	%r183, %r1, 3968;
	add.s32 	%r184, %r183, 127;
	sub.s32 	%r4, %r184, %r1;
	shr.u32 	%r185, %r4, 7;
	add.s32 	%r186, %r185, 1;
	and.b32  	%r695, %r186, 3;
	setp.eq.s32 	%p3, %r695, 0;
	mov.u32 	%r696, %r1;
	@%p3 bra 	$L__BB0_7;

	mov.u32 	%r188, smem;
	mov.u32 	%r696, %r1;

$L__BB0_6:
	.pragma "nounroll";
	shl.b32 	%r187, %r696, 2;
	add.s32 	%r189, %r188, %r187;
	mov.u32 	%r190, 0;
	st.shared.u32 	[%r189+8320], %r190;
	add.s32 	%r696, %r696, 128;
	add.s32 	%r695, %r695, -1;
	setp.ne.s32 	%p4, %r695, 0;
	@%p4 bra 	$L__BB0_6;

$L__BB0_7:
	setp.lt.u32 	%p5, %r4, 384;
	@%p5 bra 	$L__BB0_10;

	mov.u32 	%r192, smem;

$L__BB0_9:
	shl.b32 	%r191, %r696, 2;
	add.s32 	%r193, %r192, %r191;
	mov.u32 	%r194, 0;
	st.shared.u32 	[%r193+8320], %r194;
	st.shared.u32 	[%r193+8832], %r194;
	st.shared.u32 	[%r193+9344], %r194;
	st.shared.u32 	[%r193+9856], %r194;
	add.s32 	%r12, %r696, 512;
	setp.lt.s32 	%p6, %r696, 3584;
	mov.u32 	%r696, %r12;
	@%p6 bra 	$L__BB0_9;

$L__BB0_10:
	setp.gt.s32 	%p7, %r1, 31;
	bar.sync 	0;
	@%p7 bra 	$L__BB0_48;

	mov.u32 	%r195, 127;
	sub.s32 	%r13, %r195, %r1;
	shr.u32 	%r196, %r13, 5;
	add.s32 	%r197, %r196, 1;
	and.b32  	%r699, %r197, 3;
	setp.eq.s32 	%p8, %r699, 0;
	mov.u32 	%r700, %r1;
	@%p8 bra 	$L__BB0_17;

	mov.u32 	%r700, %r1;

$L__BB0_13:
	.pragma "nounroll";
	shl.b32 	%r198, %r700, 3;
	shr.s32 	%r199, %r198, 31;
	shr.u32 	%r200, %r199, 28;
	add.s32 	%r201, %r198, %r200;
	and.b32  	%r202, %r201, -16;
	sub.s32 	%r17, %r198, %r202;
	shr.u32 	%r203, %r700, 31;
	add.s32 	%r204, %r700, %r203;
	shr.s32 	%r18, %r204, 1;
	add.s32 	%r19, %r18, %r2;
	setp.gt.s32 	%p9, %r19, 63;
	@%p9 bra 	$L__BB0_16;

	and.b32  	%r205, %r19, 56;
	shr.u32 	%r206, %r205, 3;
	shr.s32 	%r207, %r17, 5;
	mul.wide.u32 	%rd14, %r207, 1431655766;
	shr.u64 	%rd15, %rd14, 32;
	cvt.u32.u64 	%r208, %rd15;
	add.s32 	%r209, %r206, %r208;
	add.s32 	%r20, %r209, -1;
	and.b32  	%r210, %r19, 7;
	add.s32 	%r211, %r210, %r207;
	add.s32 	%r212, %r211, -1;
	mad.lo.s32 	%r21, %r208, -3, %r212;
	or.b32  	%r213, %r21, %r20;
	and.b32  	%r214, %r213, -8;
	setp.ne.s32 	%p10, %r214, 0;
	@%p10 bra 	$L__BB0_16;

	and.b32  	%r216, %r17, 24;
	shl.b32 	%r217, %r18, 4;
	add.s32 	%r218, %r217, %r17;
	shr.s32 	%r219, %r19, 6;
	mul.wide.s32 	%rd17, %r219, 64;
	shl.b32 	%r220, %r20, 3;
	cvt.s64.s32 	%rd18, %r220;
	add.s64 	%rd19, %rd17, %rd18;
	cvt.s64.s32 	%rd20, %r21;
	add.s64 	%rd21, %rd19, %rd20;
	shl.b64 	%rd22, %rd21, 5;
	cvt.u64.u32 	%rd23, %r216;
	or.b64  	%rd24, %rd22, %rd23;
	shl.b64 	%rd25, %rd24, 1;
	add.s64 	%rd16, %rd10, %rd25;
	shl.b32 	%r221, %r218, 1;
	mov.u32 	%r222, smem;
	add.s32 	%r223, %r222, %r221;
	add.s32 	%r215, %r223, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r215], [%rd16], 16;
	// end inline asm

$L__BB0_16:
	add.s32 	%r700, %r700, 32;
	add.s32 	%r699, %r699, -1;
	setp.ne.s32 	%p11, %r699, 0;
	@%p11 bra 	$L__BB0_13;

$L__BB0_17:
	setp.lt.u32 	%p12, %r13, 96;
	@%p12 bra 	$L__BB0_31;

$L__BB0_18:
	.pragma "nounroll";
	mov.u32 	%r25, %r700;
	shl.b32 	%r224, %r25, 3;
	shr.s32 	%r225, %r224, 31;
	shr.u32 	%r226, %r225, 28;
	add.s32 	%r227, %r224, %r226;
	and.b32  	%r228, %r227, -16;
	sub.s32 	%r26, %r224, %r228;
	shr.u32 	%r229, %r25, 31;
	add.s32 	%r230, %r25, %r229;
	shr.s32 	%r27, %r230, 1;
	add.s32 	%r28, %r27, %r2;
	setp.gt.s32 	%p13, %r28, 63;
	@%p13 bra 	$L__BB0_21;

	and.b32  	%r231, %r28, 56;
	shr.u32 	%r232, %r231, 3;
	shr.s32 	%r233, %r26, 5;
	mul.wide.u32 	%rd26, %r233, 1431655766;
	shr.u64 	%rd27, %rd26, 32;
	cvt.u32.u64 	%r234, %rd27;
	add.s32 	%r235, %r232, %r234;
	add.s32 	%r29, %r235, -1;
	and.b32  	%r236, %r28, 7;
	add.s32 	%r237, %r236, %r233;
	add.s32 	%r238, %r237, -1;
	mad.lo.s32 	%r30, %r234, -3, %r238;
	or.b32  	%r239, %r30, %r29;
	and.b32  	%r240, %r239, -8;
	setp.ne.s32 	%p14, %r240, 0;
	@%p14 bra 	$L__BB0_21;

	and.b32  	%r242, %r26, 24;
	shl.b32 	%r243, %r27, 4;
	add.s32 	%r244, %r243, %r26;
	shr.s32 	%r245, %r28, 6;
	mul.wide.s32 	%rd29, %r245, 64;
	shl.b32 	%r246, %r29, 3;
	cvt.s64.s32 	%rd30, %r246;
	add.s64 	%rd31, %rd29, %rd30;
	cvt.s64.s32 	%rd32, %r30;
	add.s64 	%rd33, %rd31, %rd32;
	shl.b64 	%rd34, %rd33, 5;
	cvt.u64.u32 	%rd35, %r242;
	or.b64  	%rd36, %rd34, %rd35;
	shl.b64 	%rd37, %rd36, 1;
	add.s64 	%rd28, %rd10, %rd37;
	shl.b32 	%r247, %r244, 1;
	mov.u32 	%r248, smem;
	add.s32 	%r249, %r248, %r247;
	add.s32 	%r241, %r249, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r241], [%rd28], 16;
	// end inline asm

$L__BB0_21:
	add.s32 	%r250, %r25, 32;
	shr.u32 	%r251, %r250, 31;
	add.s32 	%r252, %r250, %r251;
	shr.s32 	%r31, %r252, 1;
	shl.b32 	%r253, %r250, 3;
	shr.s32 	%r254, %r253, 31;
	shr.u32 	%r255, %r254, 28;
	add.s32 	%r256, %r253, %r255;
	and.b32  	%r257, %r256, -16;
	sub.s32 	%r32, %r253, %r257;
	add.s32 	%r33, %r31, %r2;
	setp.gt.s32 	%p15, %r33, 63;
	@%p15 bra 	$L__BB0_24;

	and.b32  	%r258, %r33, 56;
	shr.u32 	%r259, %r258, 3;
	shr.s32 	%r260, %r32, 5;
	mul.wide.u32 	%rd38, %r260, 1431655766;
	shr.u64 	%rd39, %rd38, 32;
	cvt.u32.u64 	%r261, %rd39;
	add.s32 	%r262, %r259, %r261;
	add.s32 	%r34, %r262, -1;
	and.b32  	%r263, %r33, 7;
	add.s32 	%r264, %r263, %r260;
	add.s32 	%r265, %r264, -1;
	mad.lo.s32 	%r35, %r261, -3, %r265;
	or.b32  	%r266, %r35, %r34;
	and.b32  	%r267, %r266, -8;
	setp.ne.s32 	%p16, %r267, 0;
	@%p16 bra 	$L__BB0_24;

	and.b32  	%r269, %r32, 24;
	shl.b32 	%r270, %r31, 4;
	add.s32 	%r271, %r270, %r32;
	shr.s32 	%r272, %r33, 6;
	mul.wide.s32 	%rd41, %r272, 64;
	shl.b32 	%r273, %r34, 3;
	cvt.s64.s32 	%rd42, %r273;
	add.s64 	%rd43, %rd41, %rd42;
	cvt.s64.s32 	%rd44, %r35;
	add.s64 	%rd45, %rd43, %rd44;
	shl.b64 	%rd46, %rd45, 5;
	cvt.u64.u32 	%rd47, %r269;
	or.b64  	%rd48, %rd46, %rd47;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd40, %rd10, %rd49;
	shl.b32 	%r274, %r271, 1;
	mov.u32 	%r275, smem;
	add.s32 	%r276, %r275, %r274;
	add.s32 	%r268, %r276, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r268], [%rd40], 16;
	// end inline asm

$L__BB0_24:
	add.s32 	%r277, %r25, 64;
	shr.u32 	%r278, %r277, 31;
	add.s32 	%r279, %r277, %r278;
	shr.s32 	%r36, %r279, 1;
	shl.b32 	%r280, %r277, 3;
	shr.s32 	%r281, %r280, 31;
	shr.u32 	%r282, %r281, 28;
	add.s32 	%r283, %r280, %r282;
	and.b32  	%r284, %r283, -16;
	sub.s32 	%r37, %r280, %r284;
	add.s32 	%r38, %r36, %r2;
	setp.gt.s32 	%p17, %r38, 63;
	@%p17 bra 	$L__BB0_27;

	and.b32  	%r285, %r38, 56;
	shr.u32 	%r286, %r285, 3;
	shr.s32 	%r287, %r37, 5;
	mul.wide.u32 	%rd50, %r287, 1431655766;
	shr.u64 	%rd51, %rd50, 32;
	cvt.u32.u64 	%r288, %rd51;
	add.s32 	%r289, %r286, %r288;
	add.s32 	%r39, %r289, -1;
	and.b32  	%r290, %r38, 7;
	add.s32 	%r291, %r290, %r287;
	add.s32 	%r292, %r291, -1;
	mad.lo.s32 	%r40, %r288, -3, %r292;
	or.b32  	%r293, %r40, %r39;
	and.b32  	%r294, %r293, -8;
	setp.ne.s32 	%p18, %r294, 0;
	@%p18 bra 	$L__BB0_27;

	and.b32  	%r296, %r37, 24;
	shl.b32 	%r297, %r36, 4;
	add.s32 	%r298, %r297, %r37;
	shr.s32 	%r299, %r38, 6;
	mul.wide.s32 	%rd53, %r299, 64;
	shl.b32 	%r300, %r39, 3;
	cvt.s64.s32 	%rd54, %r300;
	add.s64 	%rd55, %rd53, %rd54;
	cvt.s64.s32 	%rd56, %r40;
	add.s64 	%rd57, %rd55, %rd56;
	shl.b64 	%rd58, %rd57, 5;
	cvt.u64.u32 	%rd59, %r296;
	or.b64  	%rd60, %rd58, %rd59;
	shl.b64 	%rd61, %rd60, 1;
	add.s64 	%rd52, %rd10, %rd61;
	shl.b32 	%r301, %r298, 1;
	mov.u32 	%r302, smem;
	add.s32 	%r303, %r302, %r301;
	add.s32 	%r295, %r303, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r295], [%rd52], 16;
	// end inline asm

$L__BB0_27:
	add.s32 	%r304, %r25, 96;
	shr.u32 	%r305, %r304, 31;
	add.s32 	%r306, %r304, %r305;
	shr.s32 	%r41, %r306, 1;
	shl.b32 	%r307, %r304, 3;
	shr.s32 	%r308, %r307, 31;
	shr.u32 	%r309, %r308, 28;
	add.s32 	%r310, %r307, %r309;
	and.b32  	%r311, %r310, -16;
	sub.s32 	%r42, %r307, %r311;
	add.s32 	%r43, %r41, %r2;
	setp.gt.s32 	%p19, %r43, 63;
	@%p19 bra 	$L__BB0_30;

	and.b32  	%r312, %r43, 56;
	shr.u32 	%r313, %r312, 3;
	shr.s32 	%r314, %r42, 5;
	mul.wide.u32 	%rd62, %r314, 1431655766;
	shr.u64 	%rd63, %rd62, 32;
	cvt.u32.u64 	%r315, %rd63;
	add.s32 	%r316, %r313, %r315;
	add.s32 	%r44, %r316, -1;
	and.b32  	%r317, %r43, 7;
	add.s32 	%r318, %r317, %r314;
	add.s32 	%r319, %r318, -1;
	mad.lo.s32 	%r45, %r315, -3, %r319;
	or.b32  	%r320, %r45, %r44;
	and.b32  	%r321, %r320, -8;
	setp.ne.s32 	%p20, %r321, 0;
	@%p20 bra 	$L__BB0_30;

	and.b32  	%r323, %r42, 24;
	shl.b32 	%r324, %r41, 4;
	add.s32 	%r325, %r324, %r42;
	shr.s32 	%r326, %r43, 6;
	mul.wide.s32 	%rd65, %r326, 64;
	shl.b32 	%r327, %r44, 3;
	cvt.s64.s32 	%rd66, %r327;
	add.s64 	%rd67, %rd65, %rd66;
	cvt.s64.s32 	%rd68, %r45;
	add.s64 	%rd69, %rd67, %rd68;
	shl.b64 	%rd70, %rd69, 5;
	cvt.u64.u32 	%rd71, %r323;
	or.b64  	%rd72, %rd70, %rd71;
	shl.b64 	%rd73, %rd72, 1;
	add.s64 	%rd64, %rd10, %rd73;
	shl.b32 	%r328, %r325, 1;
	mov.u32 	%r329, smem;
	add.s32 	%r330, %r329, %r328;
	add.s32 	%r322, %r330, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r322], [%rd64], 16;
	// end inline asm

$L__BB0_30:
	add.s32 	%r700, %r25, 128;
	setp.lt.s32 	%p21, %r25, 0;
	@%p21 bra 	$L__BB0_18;

$L__BB0_31:
	setp.gt.s32 	%p22, %r1, 127;
	@%p22 bra 	$L__BB0_47;

	max.s32 	%r331, %r1, 96;
	add.s32 	%r332, %r331, 31;
	sub.s32 	%r47, %r332, %r1;
	shr.u32 	%r333, %r47, 5;
	add.s32 	%r334, %r333, 1;
	and.b32  	%r703, %r334, 3;
	setp.eq.s32 	%p23, %r703, 0;
	mov.u32 	%r704, %r1;
	@%p23 bra 	$L__BB0_37;

	mov.u32 	%r704, %r1;

$L__BB0_34:
	.pragma "nounroll";
	shl.b32 	%r335, %r704, 3;
	shr.s32 	%r336, %r335, 31;
	shr.u32 	%r337, %r336, 26;
	add.s32 	%r338, %r335, %r337;
	and.b32  	%r339, %r338, -64;
	sub.s32 	%r51, %r335, %r339;
	add.s32 	%r52, %r51, %r3;
	setp.gt.s32 	%p24, %r52, 31;
	@%p24 bra 	$L__BB0_36;

	shr.s32 	%r341, %r704, 31;
	shr.u32 	%r342, %r341, 29;
	add.s32 	%r343, %r704, %r342;
	shr.s32 	%r344, %r343, 3;
	shl.b32 	%r345, %r344, 6;
	add.s32 	%r346, %r345, %r51;
	mul.wide.s32 	%rd75, %r344, 32;
	cvt.s64.s32 	%rd76, %r52;
	add.s64 	%rd77, %rd75, %rd76;
	shl.b64 	%rd78, %rd77, 1;
	add.s64 	%rd74, %rd11, %rd78;
	shl.b32 	%r347, %r346, 1;
	mov.u32 	%r348, smem;
	add.s32 	%r349, %r348, %r347;
	add.s32 	%r340, %r349, 4224;
	// begin inline asm
	cp.async.ca.shared.global [%r340], [%rd74], 16;
	// end inline asm

$L__BB0_36:
	add.s32 	%r704, %r704, 32;
	add.s32 	%r703, %r703, -1;
	setp.ne.s32 	%p25, %r703, 0;
	@%p25 bra 	$L__BB0_34;

$L__BB0_37:
	setp.lt.u32 	%p26, %r47, 96;
	@%p26 bra 	$L__BB0_47;

$L__BB0_38:
	.pragma "nounroll";
	shl.b32 	%r350, %r704, 3;
	shr.s32 	%r351, %r350, 31;
	shr.u32 	%r352, %r351, 26;
	add.s32 	%r353, %r350, %r352;
	and.b32  	%r354, %r353, -64;
	sub.s32 	%r57, %r350, %r354;
	add.s32 	%r58, %r57, %r3;
	setp.gt.s32 	%p27, %r58, 31;
	@%p27 bra 	$L__BB0_40;

	shr.s32 	%r356, %r704, 31;
	shr.u32 	%r357, %r356, 29;
	add.s32 	%r358, %r704, %r357;
	shr.s32 	%r359, %r358, 3;
	shl.b32 	%r360, %r359, 6;
	add.s32 	%r361, %r360, %r57;
	mul.wide.s32 	%rd80, %r359, 32;
	cvt.s64.s32 	%rd81, %r58;
	add.s64 	%rd82, %rd80, %rd81;
	shl.b64 	%rd83, %rd82, 1;
	add.s64 	%rd79, %rd11, %rd83;
	shl.b32 	%r362, %r361, 1;
	mov.u32 	%r363, smem;
	add.s32 	%r364, %r363, %r362;
	add.s32 	%r355, %r364, 4224;
	// begin inline asm
	cp.async.ca.shared.global [%r355], [%rd79], 16;
	// end inline asm

$L__BB0_40:
	add.s32 	%r59, %r704, 32;
	shl.b32 	%r365, %r59, 3;
	shr.s32 	%r366, %r365, 31;
	shr.u32 	%r367, %r366, 26;
	add.s32 	%r368, %r365, %r367;
	and.b32  	%r369, %r368, -64;
	sub.s32 	%r60, %r365, %r369;
	add.s32 	%r61, %r60, %r3;
	setp.gt.s32 	%p28, %r61, 31;
	@%p28 bra 	$L__BB0_42;

	shr.s32 	%r371, %r59, 31;
	shr.u32 	%r372, %r371, 29;
	add.s32 	%r373, %r59, %r372;
	shr.s32 	%r374, %r373, 3;
	shl.b32 	%r375, %r374, 6;
	add.s32 	%r376, %r375, %r60;
	mul.wide.s32 	%rd85, %r374, 32;
	cvt.s64.s32 	%rd86, %r61;
	add.s64 	%rd87, %rd85, %rd86;
	shl.b64 	%rd88, %rd87, 1;
	add.s64 	%rd84, %rd11, %rd88;
	shl.b32 	%r377, %r376, 1;
	mov.u32 	%r378, smem;
	add.s32 	%r379, %r378, %r377;
	add.s32 	%r370, %r379, 4224;
	// begin inline asm
	cp.async.ca.shared.global [%r370], [%rd84], 16;
	// end inline asm

$L__BB0_42:
	add.s32 	%r62, %r704, 64;
	shl.b32 	%r380, %r62, 3;
	shr.s32 	%r381, %r380, 31;
	shr.u32 	%r382, %r381, 26;
	add.s32 	%r383, %r380, %r382;
	and.b32  	%r384, %r383, -64;
	sub.s32 	%r63, %r380, %r384;
	add.s32 	%r64, %r63, %r3;
	setp.gt.s32 	%p29, %r64, 31;
	@%p29 bra 	$L__BB0_44;

	shr.s32 	%r386, %r62, 31;
	shr.u32 	%r387, %r386, 29;
	add.s32 	%r388, %r62, %r387;
	shr.s32 	%r389, %r388, 3;
	shl.b32 	%r390, %r389, 6;
	add.s32 	%r391, %r390, %r63;
	mul.wide.s32 	%rd90, %r389, 32;
	cvt.s64.s32 	%rd91, %r64;
	add.s64 	%rd92, %rd90, %rd91;
	shl.b64 	%rd93, %rd92, 1;
	add.s64 	%rd89, %rd11, %rd93;
	shl.b32 	%r392, %r391, 1;
	mov.u32 	%r393, smem;
	add.s32 	%r394, %r393, %r392;
	add.s32 	%r385, %r394, 4224;
	// begin inline asm
	cp.async.ca.shared.global [%r385], [%rd89], 16;
	// end inline asm

$L__BB0_44:
	add.s32 	%r65, %r704, 96;
	shl.b32 	%r395, %r65, 3;
	shr.s32 	%r396, %r395, 31;
	shr.u32 	%r397, %r396, 26;
	add.s32 	%r398, %r395, %r397;
	and.b32  	%r399, %r398, -64;
	sub.s32 	%r66, %r395, %r399;
	add.s32 	%r67, %r66, %r3;
	setp.gt.s32 	%p30, %r67, 31;
	@%p30 bra 	$L__BB0_46;

	shr.s32 	%r401, %r65, 31;
	shr.u32 	%r402, %r401, 29;
	add.s32 	%r403, %r65, %r402;
	shr.s32 	%r404, %r403, 3;
	shl.b32 	%r405, %r404, 6;
	add.s32 	%r406, %r405, %r66;
	mul.wide.s32 	%rd95, %r404, 32;
	cvt.s64.s32 	%rd96, %r67;
	add.s64 	%rd97, %rd95, %rd96;
	shl.b64 	%rd98, %rd97, 1;
	add.s64 	%rd94, %rd11, %rd98;
	shl.b32 	%r407, %r406, 1;
	mov.u32 	%r408, smem;
	add.s32 	%r409, %r408, %r407;
	add.s32 	%r400, %r409, 4224;
	// begin inline asm
	cp.async.ca.shared.global [%r400], [%rd94], 16;
	// end inline asm

$L__BB0_46:
	add.s32 	%r68, %r704, 128;
	setp.lt.s32 	%p31, %r704, 0;
	mov.u32 	%r704, %r68;
	@%p31 bra 	$L__BB0_38;

$L__BB0_47:
	// begin inline asm
	cp.async.commit_group;
	// end inline asm
	// begin inline asm
	cp.async.wait_group 0;
	// end inline asm

$L__BB0_48:
	shr.s32 	%r411, %r1, 31;
	shr.u32 	%r412, %r411, 27;
	add.s32 	%r413, %r1, %r412;
	shr.s32 	%r414, %r413, 5;
	bar.sync 	0;
	max.s32 	%r415, %r1, 96;
	add.s32 	%r416, %r415, 31;
	sub.s32 	%r417, %r416, %r1;
	shr.u32 	%r418, %r417, 5;
	add.s32 	%r419, %r418, 1;
	shl.b32 	%r420, %r1, 3;
	shr.s32 	%r421, %r420, 31;
	shr.u32 	%r422, %r421, 28;
	add.s32 	%r423, %r420, %r422;
	and.b32  	%r424, %r423, -16;
	sub.s32 	%r69, %r420, %r424;
	shr.u32 	%r425, %r1, 31;
	add.s32 	%r426, %r1, %r425;
	shr.s32 	%r427, %r426, 1;
	add.s32 	%r70, %r427, %r2;
	and.b32  	%r71, %r419, 3;
	and.b32  	%r428, %r70, 56;
	shr.u32 	%r429, %r428, 3;
	and.b32  	%r430, %r70, 7;
	add.s32 	%r72, %r429, -1;
	add.s32 	%r73, %r430, -1;
	shr.u32 	%r431, %r421, 26;
	add.s32 	%r432, %r420, %r431;
	and.b32  	%r433, %r432, -64;
	sub.s32 	%r74, %r420, %r433;
	add.s32 	%r434, %r74, %r3;
	shr.s32 	%r435, %r70, 6;
	shl.b32 	%r436, %r427, 4;
	add.s32 	%r75, %r436, %r69;
	mul.wide.s32 	%rd6, %r435, 64;
	shr.u32 	%r437, %r411, 29;
	add.s32 	%r438, %r1, %r437;
	shr.s32 	%r76, %r438, 3;
	cvt.s64.s32 	%rd7, %r434;
	add.s32 	%r439, %r1, 32;
	shr.s32 	%r440, %r439, 31;
	shr.u32 	%r441, %r440, 29;
	add.s32 	%r442, %r439, %r441;
	shr.s32 	%r77, %r442, 3;
	shl.b32 	%r443, %r439, 3;
	shr.s32 	%r444, %r443, 31;
	shr.u32 	%r445, %r444, 26;
	add.s32 	%r446, %r443, %r445;
	and.b32  	%r447, %r446, -64;
	sub.s32 	%r78, %r443, %r447;
	add.s32 	%r448, %r78, %r3;
	cvt.s64.s32 	%rd8, %r448;
	add.s32 	%r449, %r1, 64;
	shr.s32 	%r450, %r449, 31;
	shr.u32 	%r451, %r450, 29;
	add.s32 	%r452, %r449, %r451;
	shr.s32 	%r79, %r452, 3;
	shl.b32 	%r453, %r449, 3;
	shr.s32 	%r454, %r453, 31;
	shr.u32 	%r455, %r454, 26;
	add.s32 	%r456, %r453, %r455;
	and.b32  	%r457, %r456, -64;
	sub.s32 	%r80, %r453, %r457;
	add.s32 	%r458, %r80, %r3;
	cvt.s64.s32 	%rd9, %r458;
	add.s32 	%r81, %r414, -4;
	add.s32 	%r82, %r1, -32;
	mov.u32 	%r706, 0;

$L__BB0_49:
	setp.lt.s32 	%p32, %r1, 32;
	@%p32 bra 	$L__BB0_58;
	bra.uni 	$L__BB0_50;

$L__BB0_58:
	add.s32 	%r94, %r706, 1;
	setp.lt.u32 	%p38, %r94, 18;
	@%p38 bra 	$L__BB0_60;
	bra.uni 	$L__BB0_59;

$L__BB0_60:
	setp.gt.s32 	%p39, %r1, 127;
	shl.b32 	%r95, %r94, 4;
	@%p39 bra 	$L__BB0_76;

	shl.b32 	%r481, %r94, 11;
	and.b32  	%r482, %r481, 2048;
	mov.u32 	%r483, smem;
	add.s32 	%r484, %r483, %r482;
	add.s32 	%r96, %r484, 128;
	mov.u32 	%r711, %tid.x;
	max.s32 	%r485, %r711, 96;
	add.s32 	%r486, %r485, 31;
	sub.s32 	%r487, %r486, %r711;
	shr.u32 	%r488, %r487, 5;
	add.s32 	%r489, %r488, 1;
	and.b32  	%r490, %r489, 1;
	setp.eq.b32 	%p40, %r490, 1;
	mov.pred 	%p41, 0;
	xor.pred  	%p42, %p40, %p41;
	not.pred 	%p43, %p42;
	@%p43 bra 	$L__BB0_66;

	setp.gt.s32 	%p44, %r70, 63;
	add.s32 	%r98, %r69, %r95;
	setp.gt.s32 	%p45, %r98, 287;
	mov.u32 	%r491, %tid.x;
	add.s32 	%r711, %r491, 32;
	or.pred  	%p46, %p44, %p45;
	@%p46 bra 	$L__BB0_66;

	shr.s32 	%r492, %r98, 5;
	mul.wide.u32 	%rd99, %r492, 1431655766;
	shr.u64 	%rd100, %rd99, 32;
	cvt.u32.u64 	%r493, %rd100;
	add.s32 	%r100, %r72, %r493;
	add.s32 	%r494, %r73, %r492;
	mad.lo.s32 	%r101, %r493, -3, %r494;
	or.b32  	%r495, %r101, %r100;
	and.b32  	%r496, %r495, -8;
	setp.eq.s32 	%p47, %r496, 0;
	@%p47 bra 	$L__BB0_65;

	shl.b32 	%r497, %r75, 1;
	add.s32 	%r498, %r96, %r497;
	mov.u32 	%r499, 0;
	st.shared.v4.u32 	[%r498], {%r499, %r499, %r499, %r499};
	bra.uni 	$L__BB0_66;

$L__BB0_50:
	shl.b32 	%r460, %r706, 11;
	and.b32  	%r461, %r460, 2048;
	mov.u32 	%r462, smem;
	add.s32 	%r463, %r462, %r461;
	add.s32 	%r84, %r463, 128;
	add.s32 	%r85, %r463, 4224;
	shl.b32 	%r86, %r706, 4;
	mov.u32 	%r707, 0;

$L__BB0_51:
	add.s32 	%r464, %r707, %r86;
	setp.gt.s32 	%p33, %r464, 287;
	@%p33 bra 	$L__BB0_97;

	setp.gt.s32 	%p34, %r1, 4127;
	@%p34 bra 	$L__BB0_57;

	shl.b32 	%r88, %r707, 6;
	mov.u32 	%r708, %r82;
	mov.u32 	%r709, %r81;

$L__BB0_54:
	setp.gt.s32 	%p35, %r708, 4095;
	@%p35 bra 	$L__BB0_56;

	shr.s32 	%r465, %r708, 31;
	shr.u32 	%r466, %r465, 26;
	add.s32 	%r467, %r708, %r466;
	shr.s32 	%r468, %r467, 6;
	and.b32  	%r469, %r467, -64;
	sub.s32 	%r470, %r708, %r469;
	shl.b32 	%r471, %r468, 4;
	add.s32 	%r472, %r471, %r707;
	shl.b32 	%r473, %r472, 1;
	add.s32 	%r474, %r84, %r473;
	ld.shared.u16 	%rs1, [%r474];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	add.s32 	%r475, %r470, %r88;
	shl.b32 	%r476, %r475, 1;
	add.s32 	%r477, %r85, %r476;
	ld.shared.u16 	%rs2, [%r477];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	shl.b32 	%r478, %r708, 2;
	add.s32 	%r480, %r462, %r478;
	ld.shared.f32 	%f3, [%r480+8320];
	fma.rn.f32 	%f4, %f1, %f2, %f3;
	st.shared.f32 	[%r480+8320], %f4;

$L__BB0_56:
	add.s32 	%r709, %r709, 3;
	add.s32 	%r708, %r708, 96;
	setp.lt.s32 	%p36, %r709, 125;
	@%p36 bra 	$L__BB0_54;

$L__BB0_57:
	add.s32 	%r707, %r707, 1;
	setp.lt.u32 	%p37, %r707, 16;
	@%p37 bra 	$L__BB0_51;
	bra.uni 	$L__BB0_97;

$L__BB0_59:
	// begin inline asm
	cp.async.wait_group 0;
	// end inline asm
	bra.uni 	$L__BB0_97;

$L__BB0_65:
	shl.b32 	%r502, %r100, 3;
	cvt.s64.s32 	%rd102, %r502;
	add.s64 	%rd103, %rd6, %rd102;
	cvt.s64.s32 	%rd104, %r101;
	add.s64 	%rd105, %rd103, %rd104;
	shl.b64 	%rd106, %rd105, 5;
	and.b32  	%r503, %r98, 24;
	cvt.u64.u32 	%rd107, %r503;
	or.b64  	%rd108, %rd106, %rd107;
	shl.b64 	%rd109, %rd108, 1;
	add.s64 	%rd101, %rd10, %rd109;
	shl.b32 	%r504, %r75, 1;
	add.s32 	%r501, %r96, %r504;
	// begin inline asm
	cp.async.ca.shared.global [%r501], [%rd101], 16;
	// end inline asm

$L__BB0_66:
	setp.eq.s32 	%p48, %r488, 0;
	@%p48 bra 	$L__BB0_76;

$L__BB0_67:
	.pragma "nounroll";
	mov.u32 	%r105, %r711;
	shl.b32 	%r511, %r105, 3;
	shr.s32 	%r512, %r511, 31;
	shr.u32 	%r513, %r512, 28;
	add.s32 	%r514, %r511, %r513;
	and.b32  	%r515, %r514, -16;
	sub.s32 	%r106, %r511, %r515;
	shr.u32 	%r516, %r105, 31;
	add.s32 	%r517, %r105, %r516;
	shr.s32 	%r107, %r517, 1;
	add.s32 	%r108, %r107, %r2;
	add.s32 	%r109, %r106, %r95;
	setp.gt.s32 	%p49, %r108, 63;
	setp.gt.s32 	%p50, %r109, 287;
	or.pred  	%p51, %p49, %p50;
	@%p51 bra 	$L__BB0_71;

	and.b32  	%r518, %r108, 56;
	shr.u32 	%r519, %r518, 3;
	shr.s32 	%r520, %r109, 5;
	mul.wide.u32 	%rd110, %r520, 1431655766;
	shr.u64 	%rd111, %rd110, 32;
	cvt.u32.u64 	%r521, %rd111;
	add.s32 	%r522, %r519, %r521;
	add.s32 	%r110, %r522, -1;
	and.b32  	%r523, %r108, 7;
	add.s32 	%r524, %r523, %r520;
	add.s32 	%r525, %r524, -1;
	mad.lo.s32 	%r111, %r521, -3, %r525;
	or.b32  	%r526, %r111, %r110;
	and.b32  	%r527, %r526, -8;
	setp.eq.s32 	%p52, %r527, 0;
	@%p52 bra 	$L__BB0_70;

	shl.b32 	%r528, %r107, 4;
	add.s32 	%r529, %r528, %r106;
	shl.b32 	%r530, %r529, 1;
	add.s32 	%r531, %r96, %r530;
	mov.u32 	%r532, 0;
	st.shared.v4.u32 	[%r531], {%r532, %r532, %r532, %r532};
	bra.uni 	$L__BB0_71;

$L__BB0_70:
	shl.b32 	%r534, %r107, 4;
	add.s32 	%r535, %r534, %r106;
	shr.s32 	%r536, %r108, 6;
	mul.wide.s32 	%rd113, %r536, 64;
	shl.b32 	%r537, %r110, 3;
	cvt.s64.s32 	%rd114, %r537;
	add.s64 	%rd115, %rd113, %rd114;
	cvt.s64.s32 	%rd116, %r111;
	add.s64 	%rd117, %rd115, %rd116;
	shl.b64 	%rd118, %rd117, 5;
	and.b32  	%r538, %r109, 24;
	cvt.u64.u32 	%rd119, %r538;
	or.b64  	%rd120, %rd118, %rd119;
	shl.b64 	%rd121, %rd120, 1;
	add.s64 	%rd112, %rd10, %rd121;
	shl.b32 	%r539, %r535, 1;
	add.s32 	%r533, %r96, %r539;
	// begin inline asm
	cp.async.ca.shared.global [%r533], [%rd112], 16;
	// end inline asm

$L__BB0_71:
	add.s32 	%r540, %r105, 32;
	shr.u32 	%r541, %r540, 31;
	add.s32 	%r542, %r540, %r541;
	shr.s32 	%r112, %r542, 1;
	shl.b32 	%r543, %r540, 3;
	shr.s32 	%r544, %r543, 31;
	shr.u32 	%r545, %r544, 28;
	add.s32 	%r546, %r543, %r545;
	and.b32  	%r547, %r546, -16;
	sub.s32 	%r113, %r543, %r547;
	add.s32 	%r114, %r112, %r2;
	add.s32 	%r115, %r113, %r95;
	setp.gt.s32 	%p53, %r114, 63;
	setp.gt.s32 	%p54, %r115, 287;
	or.pred  	%p55, %p53, %p54;
	@%p55 bra 	$L__BB0_75;

	and.b32  	%r548, %r114, 56;
	shr.u32 	%r549, %r548, 3;
	shr.s32 	%r550, %r115, 5;
	mul.wide.u32 	%rd122, %r550, 1431655766;
	shr.u64 	%rd123, %rd122, 32;
	cvt.u32.u64 	%r551, %rd123;
	add.s32 	%r552, %r549, %r551;
	add.s32 	%r116, %r552, -1;
	and.b32  	%r553, %r114, 7;
	add.s32 	%r554, %r553, %r550;
	add.s32 	%r555, %r554, -1;
	mad.lo.s32 	%r117, %r551, -3, %r555;
	or.b32  	%r556, %r117, %r116;
	and.b32  	%r557, %r556, -8;
	setp.eq.s32 	%p56, %r557, 0;
	@%p56 bra 	$L__BB0_74;

	shl.b32 	%r558, %r112, 4;
	add.s32 	%r559, %r558, %r113;
	shl.b32 	%r560, %r559, 1;
	add.s32 	%r561, %r96, %r560;
	mov.u32 	%r562, 0;
	st.shared.v4.u32 	[%r561], {%r562, %r562, %r562, %r562};
	bra.uni 	$L__BB0_75;

$L__BB0_74:
	shl.b32 	%r564, %r112, 4;
	add.s32 	%r565, %r564, %r113;
	shr.s32 	%r566, %r114, 6;
	mul.wide.s32 	%rd125, %r566, 64;
	shl.b32 	%r567, %r116, 3;
	cvt.s64.s32 	%rd126, %r567;
	add.s64 	%rd127, %rd125, %rd126;
	cvt.s64.s32 	%rd128, %r117;
	add.s64 	%rd129, %rd127, %rd128;
	shl.b64 	%rd130, %rd129, 5;
	and.b32  	%r568, %r115, 24;
	cvt.u64.u32 	%rd131, %r568;
	or.b64  	%rd132, %rd130, %rd131;
	shl.b64 	%rd133, %rd132, 1;
	add.s64 	%rd124, %rd10, %rd133;
	shl.b32 	%r569, %r565, 1;
	add.s32 	%r563, %r96, %r569;
	// begin inline asm
	cp.async.ca.shared.global [%r563], [%rd124], 16;
	// end inline asm

$L__BB0_75:
	add.s32 	%r711, %r105, 64;
	setp.lt.s32 	%p57, %r105, 64;
	@%p57 bra 	$L__BB0_67;

$L__BB0_76:
	@%p39 bra 	$L__BB0_96;

	setp.eq.s32 	%p59, %r71, 0;
	shl.b32 	%r570, %r706, 11;
	add.s32 	%r571, %r570, 2048;
	and.b32  	%r572, %r571, 2048;
	mov.u32 	%r573, smem;
	add.s32 	%r574, %r573, %r572;
	add.s32 	%r119, %r574, 4224;
	mov.u32 	%r712, %r1;
	@%p59 bra 	$L__BB0_86;

	cvt.u32.u64 	%r575, %rd7;
	setp.gt.s32 	%p60, %r575, 31;
	add.s32 	%r120, %r76, %r95;
	setp.gt.s32 	%p61, %r120, 287;
	or.pred  	%p62, %p61, %p60;
	@%p62 bra 	$L__BB0_80;

	mul.wide.s32 	%rd135, %r120, 32;
	add.s64 	%rd136, %rd135, %rd7;
	shl.b64 	%rd137, %rd136, 1;
	add.s64 	%rd134, %rd11, %rd137;
	shl.b32 	%r577, %r76, 6;
	add.s32 	%r578, %r577, %r74;
	shl.b32 	%r579, %r578, 1;
	add.s32 	%r576, %r119, %r579;
	// begin inline asm
	cp.async.ca.shared.global [%r576], [%rd134], 16;
	// end inline asm

$L__BB0_80:
	setp.eq.s32 	%p63, %r71, 1;
	mov.u32 	%r580, %tid.x;
	add.s32 	%r712, %r580, 32;
	@%p63 bra 	$L__BB0_86;

	cvt.u32.u64 	%r581, %rd8;
	setp.gt.s32 	%p64, %r581, 31;
	add.s32 	%r122, %r77, %r95;
	setp.gt.s32 	%p65, %r122, 287;
	or.pred  	%p66, %p65, %p64;
	@%p66 bra 	$L__BB0_83;

	mul.wide.s32 	%rd139, %r122, 32;
	add.s64 	%rd140, %rd139, %rd8;
	shl.b64 	%rd141, %rd140, 1;
	add.s64 	%rd138, %rd11, %rd141;
	shl.b32 	%r583, %r77, 6;
	add.s32 	%r584, %r583, %r78;
	shl.b32 	%r585, %r584, 1;
	add.s32 	%r582, %r119, %r585;
	// begin inline asm
	cp.async.ca.shared.global [%r582], [%rd138], 16;
	// end inline asm

$L__BB0_83:
	setp.eq.s32 	%p67, %r71, 2;
	add.s32 	%r712, %r580, 64;
	@%p67 bra 	$L__BB0_86;

	cvt.u32.u64 	%r587, %rd9;
	setp.gt.s32 	%p68, %r587, 31;
	add.s32 	%r124, %r79, %r95;
	setp.gt.s32 	%p69, %r124, 287;
	add.s32 	%r712, %r580, 96;
	or.pred  	%p70, %p69, %p68;
	@%p70 bra 	$L__BB0_86;

	mul.wide.s32 	%rd143, %r124, 32;
	add.s64 	%rd144, %rd143, %rd9;
	shl.b64 	%rd145, %rd144, 1;
	add.s64 	%rd142, %rd11, %rd145;
	shl.b32 	%r590, %r79, 6;
	add.s32 	%r591, %r590, %r80;
	shl.b32 	%r592, %r591, 1;
	add.s32 	%r589, %r119, %r592;
	// begin inline asm
	cp.async.ca.shared.global [%r589], [%rd142], 16;
	// end inline asm

$L__BB0_86:
	setp.lt.u32 	%p71, %r417, 96;
	@%p71 bra 	$L__BB0_96;

$L__BB0_87:
	.pragma "nounroll";
	shl.b32 	%r598, %r712, 3;
	shr.s32 	%r599, %r598, 31;
	shr.u32 	%r600, %r599, 26;
	add.s32 	%r601, %r598, %r600;
	and.b32  	%r602, %r601, -64;
	sub.s32 	%r129, %r598, %r602;
	shr.s32 	%r603, %r712, 31;
	shr.u32 	%r604, %r603, 29;
	add.s32 	%r605, %r712, %r604;
	shr.s32 	%r130, %r605, 3;
	add.s32 	%r131, %r130, %r95;
	add.s32 	%r132, %r129, %r3;
	setp.gt.s32 	%p72, %r131, 287;
	setp.gt.s32 	%p73, %r132, 31;
	or.pred  	%p74, %p72, %p73;
	@%p74 bra 	$L__BB0_89;

	shl.b32 	%r607, %r130, 6;
	add.s32 	%r608, %r607, %r129;
	mul.wide.s32 	%rd147, %r131, 32;
	cvt.s64.s32 	%rd148, %r132;
	add.s64 	%rd149, %rd147, %rd148;
	shl.b64 	%rd150, %rd149, 1;
	add.s64 	%rd146, %rd11, %rd150;
	shl.b32 	%r609, %r608, 1;
	add.s32 	%r606, %r119, %r609;
	// begin inline asm
	cp.async.ca.shared.global [%r606], [%rd146], 16;
	// end inline asm

$L__BB0_89:
	add.s32 	%r610, %r712, 32;
	shr.s32 	%r611, %r610, 31;
	shr.u32 	%r612, %r611, 29;
	add.s32 	%r613, %r610, %r612;
	shr.s32 	%r133, %r613, 3;
	shl.b32 	%r614, %r610, 3;
	shr.s32 	%r615, %r614, 31;
	shr.u32 	%r616, %r615, 26;
	add.s32 	%r617, %r614, %r616;
	and.b32  	%r618, %r617, -64;
	sub.s32 	%r134, %r614, %r618;
	add.s32 	%r135, %r133, %r95;
	add.s32 	%r136, %r134, %r3;
	setp.gt.s32 	%p75, %r135, 287;
	setp.gt.s32 	%p76, %r136, 31;
	or.pred  	%p77, %p75, %p76;
	@%p77 bra 	$L__BB0_91;

	shl.b32 	%r620, %r133, 6;
	add.s32 	%r621, %r620, %r134;
	mul.wide.s32 	%rd152, %r135, 32;
	cvt.s64.s32 	%rd153, %r136;
	add.s64 	%rd154, %rd152, %rd153;
	shl.b64 	%rd155, %rd154, 1;
	add.s64 	%rd151, %rd11, %rd155;
	shl.b32 	%r622, %r621, 1;
	add.s32 	%r619, %r119, %r622;
	// begin inline asm
	cp.async.ca.shared.global [%r619], [%rd151], 16;
	// end inline asm

$L__BB0_91:
	add.s32 	%r623, %r712, 64;
	shr.s32 	%r624, %r623, 31;
	shr.u32 	%r625, %r624, 29;
	add.s32 	%r626, %r623, %r625;
	shr.s32 	%r137, %r626, 3;
	shl.b32 	%r627, %r623, 3;
	shr.s32 	%r628, %r627, 31;
	shr.u32 	%r629, %r628, 26;
	add.s32 	%r630, %r627, %r629;
	and.b32  	%r631, %r630, -64;
	sub.s32 	%r138, %r627, %r631;
	add.s32 	%r139, %r137, %r95;
	add.s32 	%r140, %r138, %r3;
	setp.gt.s32 	%p78, %r139, 287;
	setp.gt.s32 	%p79, %r140, 31;
	or.pred  	%p80, %p78, %p79;
	@%p80 bra 	$L__BB0_93;

	shl.b32 	%r633, %r137, 6;
	add.s32 	%r634, %r633, %r138;
	mul.wide.s32 	%rd157, %r139, 32;
	cvt.s64.s32 	%rd158, %r140;
	add.s64 	%rd159, %rd157, %rd158;
	shl.b64 	%rd160, %rd159, 1;
	add.s64 	%rd156, %rd11, %rd160;
	shl.b32 	%r635, %r634, 1;
	add.s32 	%r632, %r119, %r635;
	// begin inline asm
	cp.async.ca.shared.global [%r632], [%rd156], 16;
	// end inline asm

$L__BB0_93:
	add.s32 	%r636, %r712, 96;
	shr.s32 	%r637, %r636, 31;
	shr.u32 	%r638, %r637, 29;
	add.s32 	%r639, %r636, %r638;
	shr.s32 	%r141, %r639, 3;
	shl.b32 	%r640, %r636, 3;
	shr.s32 	%r641, %r640, 31;
	shr.u32 	%r642, %r641, 26;
	add.s32 	%r643, %r640, %r642;
	and.b32  	%r644, %r643, -64;
	sub.s32 	%r142, %r640, %r644;
	add.s32 	%r143, %r141, %r95;
	add.s32 	%r144, %r142, %r3;
	setp.gt.s32 	%p81, %r143, 287;
	setp.gt.s32 	%p82, %r144, 31;
	or.pred  	%p83, %p81, %p82;
	@%p83 bra 	$L__BB0_95;

	shl.b32 	%r646, %r141, 6;
	add.s32 	%r647, %r646, %r142;
	mul.wide.s32 	%rd162, %r143, 32;
	cvt.s64.s32 	%rd163, %r144;
	add.s64 	%rd164, %rd162, %rd163;
	shl.b64 	%rd165, %rd164, 1;
	add.s64 	%rd161, %rd11, %rd165;
	shl.b32 	%r648, %r647, 1;
	add.s32 	%r645, %r119, %r648;
	// begin inline asm
	cp.async.ca.shared.global [%r645], [%rd161], 16;
	// end inline asm

$L__BB0_95:
	add.s32 	%r145, %r712, 128;
	setp.lt.s32 	%p84, %r712, 0;
	mov.u32 	%r712, %r145;
	@%p84 bra 	$L__BB0_87;

$L__BB0_96:
	// begin inline asm
	cp.async.commit_group;
	// end inline asm
	// begin inline asm
	cp.async.wait_group 0;
	// end inline asm

$L__BB0_97:
	bar.sync 	0;
	add.s32 	%r706, %r706, 1;
	setp.lt.u32 	%p85, %r706, 18;
	@%p85 bra 	$L__BB0_49;

	setp.lt.s32 	%p86, %r1, 4128;
	xor.pred  	%p88, %p32, %p86;
	not.pred 	%p89, %p88;
	@%p89 bra 	$L__BB0_115;

	add.s32 	%r717, %r1, -32;
	max.s32 	%r649, %r717, 4000;
	add.s32 	%r650, %r649, 127;
	sub.s32 	%r148, %r650, %r1;
	mul.wide.u32 	%rd166, %r148, -1431655765;
	shr.u64 	%rd167, %rd166, 38;
	cvt.u32.u64 	%r651, %rd167;
	add.s32 	%r652, %r651, 1;
	and.b32  	%r716, %r652, 3;
	setp.eq.s32 	%p90, %r716, 0;
	@%p90 bra 	$L__BB0_104;

	shl.b32 	%r653, %r1, 2;
	mov.u32 	%r654, smem;
	add.s32 	%r655, %r654, %r653;
	add.s32 	%r714, %r655, 8192;
	cvta.to.global.u64 	%rd171, %rd12;

$L__BB0_101:
	.pragma "nounroll";
	shr.s32 	%r656, %r717, 31;
	shr.u32 	%r657, %r656, 26;
	add.s32 	%r658, %r717, %r657;
	shr.s32 	%r659, %r658, 6;
	add.s32 	%r154, %r659, %r2;
	and.b32  	%r660, %r658, -64;
	sub.s32 	%r661, %r717, %r660;
	add.s32 	%r155, %r661, %r3;
	setp.gt.s32 	%p91, %r154, 63;
	setp.gt.s32 	%p92, %r155, 31;
	or.pred  	%p93, %p91, %p92;
	@%p93 bra 	$L__BB0_103;

	ld.shared.f32 	%f5, [%r714];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f5;}

	// end inline asm
	mul.wide.s32 	%rd168, %r154, 32;
	cvt.s64.s32 	%rd169, %r155;
	add.s64 	%rd170, %rd168, %rd169;
	shl.b64 	%rd172, %rd170, 1;
	add.s64 	%rd173, %rd171, %rd172;
	st.global.u16 	[%rd173], %rs3;

$L__BB0_103:
	add.s32 	%r717, %r717, 96;
	add.s32 	%r714, %r714, 384;
	add.s32 	%r716, %r716, -1;
	setp.ne.s32 	%p94, %r716, 0;
	@%p94 bra 	$L__BB0_101;

$L__BB0_104:
	setp.lt.u32 	%p95, %r148, 288;
	@%p95 bra 	$L__BB0_115;

	shl.b32 	%r662, %r717, 2;
	mov.u32 	%r663, smem;
	add.s32 	%r664, %r663, %r662;
	add.s32 	%r718, %r664, 9472;
	cvta.to.global.u64 	%rd177, %rd12;

$L__BB0_106:
	shr.s32 	%r665, %r717, 31;
	shr.u32 	%r666, %r665, 26;
	add.s32 	%r667, %r717, %r666;
	shr.s32 	%r668, %r667, 6;
	add.s32 	%r163, %r668, %r2;
	and.b32  	%r669, %r667, -64;
	sub.s32 	%r670, %r717, %r669;
	add.s32 	%r164, %r670, %r3;
	setp.gt.s32 	%p96, %r163, 63;
	setp.gt.s32 	%p97, %r164, 31;
	or.pred  	%p98, %p96, %p97;
	@%p98 bra 	$L__BB0_108;

	add.s32 	%r690, %r718, -1152;
	ld.shared.f32 	%f6, [%r690];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f6;}

	// end inline asm
	mul.wide.s32 	%rd174, %r163, 32;
	cvt.s64.s32 	%rd175, %r164;
	add.s64 	%rd176, %rd174, %rd175;
	shl.b64 	%rd178, %rd176, 1;
	add.s64 	%rd179, %rd177, %rd178;
	st.global.u16 	[%rd179], %rs4;

$L__BB0_108:
	add.s32 	%r166, %r717, 96;
	shr.s32 	%r671, %r166, 31;
	shr.u32 	%r672, %r671, 26;
	add.s32 	%r673, %r166, %r672;
	shr.s32 	%r674, %r673, 6;
	and.b32  	%r675, %r673, -64;
	sub.s32 	%r676, %r166, %r675;
	add.s32 	%r167, %r674, %r2;
	add.s32 	%r168, %r676, %r3;
	setp.gt.s32 	%p99, %r167, 63;
	setp.gt.s32 	%p100, %r168, 31;
	or.pred  	%p101, %p99, %p100;
	@%p101 bra 	$L__BB0_110;

	add.s32 	%r691, %r718, -1152;
	ld.shared.f32 	%f7, [%r691+384];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f7;}

	// end inline asm
	mul.wide.s32 	%rd180, %r167, 32;
	cvt.s64.s32 	%rd181, %r168;
	add.s64 	%rd182, %rd180, %rd181;
	shl.b64 	%rd184, %rd182, 1;
	add.s64 	%rd185, %rd177, %rd184;
	st.global.u16 	[%rd185], %rs5;

$L__BB0_110:
	add.s32 	%r169, %r166, 96;
	shr.s32 	%r677, %r169, 31;
	shr.u32 	%r678, %r677, 26;
	add.s32 	%r679, %r169, %r678;
	shr.s32 	%r680, %r679, 6;
	and.b32  	%r681, %r679, -64;
	sub.s32 	%r682, %r169, %r681;
	add.s32 	%r170, %r680, %r2;
	add.s32 	%r171, %r682, %r3;
	setp.gt.s32 	%p102, %r170, 63;
	setp.gt.s32 	%p103, %r171, 31;
	or.pred  	%p104, %p102, %p103;
	@%p104 bra 	$L__BB0_112;

	add.s32 	%r692, %r718, -1152;
	ld.shared.f32 	%f8, [%r692+768];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f8;}

	// end inline asm
	mul.wide.s32 	%rd186, %r170, 32;
	cvt.s64.s32 	%rd187, %r171;
	add.s64 	%rd188, %rd186, %rd187;
	shl.b64 	%rd190, %rd188, 1;
	add.s64 	%rd191, %rd177, %rd190;
	st.global.u16 	[%rd191], %rs6;

$L__BB0_112:
	add.s32 	%r683, %r169, 96;
	shr.s32 	%r684, %r683, 31;
	shr.u32 	%r685, %r684, 26;
	add.s32 	%r686, %r683, %r685;
	shr.s32 	%r687, %r686, 6;
	and.b32  	%r688, %r686, -64;
	sub.s32 	%r689, %r683, %r688;
	add.s32 	%r172, %r687, %r2;
	add.s32 	%r173, %r689, %r3;
	setp.gt.s32 	%p105, %r172, 63;
	setp.gt.s32 	%p106, %r173, 31;
	or.pred  	%p107, %p105, %p106;
	@%p107 bra 	$L__BB0_114;

	add.s32 	%r693, %r718, -1152;
	ld.shared.f32 	%f9, [%r693+1152];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f9;}

	// end inline asm
	mul.wide.s32 	%rd192, %r172, 32;
	cvt.s64.s32 	%rd193, %r173;
	add.s64 	%rd194, %rd192, %rd193;
	shl.b64 	%rd196, %rd194, 1;
	add.s64 	%rd197, %rd177, %rd196;
	st.global.u16 	[%rd197], %rs7;

$L__BB0_114:
	add.s32 	%r718, %r718, 1536;
	add.s32 	%r175, %r717, 384;
	setp.lt.s32 	%p108, %r717, 3712;
	mov.u32 	%r717, %r175;
	@%p108 bra 	$L__BB0_106;

$L__BB0_115:
	ret;

}

