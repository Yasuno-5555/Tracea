//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36836380
// Cuda compilation tools, release 13.1, V13.1.80
// Based on NVVM 7.0.1
//

.version 9.1
.target sm_80
.address_size 64

	// .globl	conv2d_implicit_gemm
.extern .shared .align 16 .b8 smem[];

.visible .entry conv2d_implicit_gemm(
	.param .u64 conv2d_implicit_gemm_param_0,
	.param .u64 conv2d_implicit_gemm_param_1,
	.param .u64 conv2d_implicit_gemm_param_2
)
.maxntid 160, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<137>;
	.reg .b16 	%rs<6>;
	.reg .f32 	%f<102>;
	.reg .b32 	%r<845>;
	.reg .b64 	%rd<228>;


	ld.param.u64 	%rd9, [conv2d_implicit_gemm_param_0];
	ld.param.u64 	%rd10, [conv2d_implicit_gemm_param_1];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r188, %ctaid.x;
	mul.wide.u32 	%rd1, %r188, 101;
	mov.u32 	%r189, %nctaid.x;
	cvt.u64.u32 	%rd2, %r189;
	and.b64  	%rd12, %rd1, 545460846592;
	setp.eq.s64 	%p1, %rd12, 0;
	@%p1 bra 	$L__BB0_2;

	rem.u64 	%rd227, %rd1, %rd2;
	bra.uni 	$L__BB0_3;

$L__BB0_2:
	cvt.u32.u64 	%r190, %rd2;
	cvt.u32.u64 	%r191, %rd1;
	rem.u32 	%r192, %r191, %r190;
	cvt.u64.u32 	%rd227, %r192;

$L__BB0_3:
	cvt.u32.u64 	%r193, %rd227;
	shl.b32 	%r2, %r193, 6;
	mov.u32 	%r194, %ctaid.y;
	shl.b32 	%r3, %r194, 5;
	shr.s32 	%r195, %r1, 31;
	shr.u32 	%r196, %r195, 27;
	add.s32 	%r197, %r1, %r196;
	shr.s32 	%r198, %r197, 5;
	add.s32 	%r4, %r198, -1;
	setp.gt.s32 	%p2, %r1, 31;
	@%p2 bra 	$L__BB0_41;

	mov.u32 	%r199, 127;
	sub.s32 	%r5, %r199, %r1;
	shr.u32 	%r200, %r5, 5;
	add.s32 	%r201, %r200, 1;
	and.b32  	%r826, %r201, 3;
	setp.eq.s32 	%p3, %r826, 0;
	mov.u32 	%r827, %r1;
	@%p3 bra 	$L__BB0_10;

	mov.u32 	%r827, %r1;

$L__BB0_6:
	.pragma "nounroll";
	shl.b32 	%r202, %r827, 3;
	shr.s32 	%r203, %r202, 31;
	shr.u32 	%r204, %r203, 28;
	add.s32 	%r205, %r202, %r204;
	and.b32  	%r206, %r205, -16;
	sub.s32 	%r9, %r202, %r206;
	shr.u32 	%r207, %r827, 31;
	add.s32 	%r208, %r827, %r207;
	shr.s32 	%r10, %r208, 1;
	add.s32 	%r11, %r10, %r2;
	setp.gt.s32 	%p4, %r11, 100351;
	@%p4 bra 	$L__BB0_9;

	mad.lo.s32 	%r209, %r11, -3135, %r9;
	add.s32 	%r12, %r209, -1;
	shl.b32 	%r210, %r9, 1;
	neg.s32 	%r211, %r210;
	mad.lo.s32 	%r212, %r11, 172425, %r211;
	add.s32 	%r13, %r212, -1;
	setp.gt.u32 	%p5, %r12, 55;
	setp.gt.u32 	%p6, %r13, 55;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	$L__BB0_9;

	mul.lo.s32 	%r214, %r9, -63;
	mad.lo.s32 	%r215, %r10, 24, %r9;
	mul.wide.s32 	%rd14, %r11, 3136;
	mul.lo.s32 	%r216, %r12, 56;
	cvt.s64.s32 	%rd15, %r216;
	cvt.s64.s32 	%rd16, %r13;
	add.s64 	%rd17, %rd14, %rd16;
	add.s64 	%rd18, %rd17, %rd15;
	shl.b64 	%rd19, %rd18, 6;
	cvt.s64.s32 	%rd20, %r214;
	add.s64 	%rd21, %rd19, %rd20;
	shl.b64 	%rd22, %rd21, 1;
	add.s64 	%rd13, %rd9, %rd22;
	shl.b32 	%r217, %r215, 1;
	mov.u32 	%r218, smem;
	add.s32 	%r219, %r218, %r217;
	add.s32 	%r213, %r219, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r213], [%rd13], 16;
	// end inline asm

$L__BB0_9:
	add.s32 	%r827, %r827, 32;
	add.s32 	%r826, %r826, -1;
	setp.ne.s32 	%p8, %r826, 0;
	@%p8 bra 	$L__BB0_6;

$L__BB0_10:
	setp.lt.u32 	%p9, %r5, 96;
	@%p9 bra 	$L__BB0_24;

$L__BB0_11:
	.pragma "nounroll";
	mov.u32 	%r17, %r827;
	shl.b32 	%r220, %r17, 3;
	shr.s32 	%r221, %r220, 31;
	shr.u32 	%r222, %r221, 28;
	add.s32 	%r223, %r220, %r222;
	and.b32  	%r224, %r223, -16;
	sub.s32 	%r18, %r220, %r224;
	shr.u32 	%r225, %r17, 31;
	add.s32 	%r226, %r17, %r225;
	shr.s32 	%r19, %r226, 1;
	add.s32 	%r20, %r19, %r2;
	setp.gt.s32 	%p10, %r20, 100351;
	@%p10 bra 	$L__BB0_14;

	mad.lo.s32 	%r227, %r20, -3135, %r18;
	add.s32 	%r21, %r227, -1;
	shl.b32 	%r228, %r18, 1;
	neg.s32 	%r229, %r228;
	mad.lo.s32 	%r230, %r20, 172425, %r229;
	add.s32 	%r22, %r230, -1;
	setp.gt.u32 	%p11, %r21, 55;
	setp.gt.u32 	%p12, %r22, 55;
	or.pred  	%p13, %p12, %p11;
	@%p13 bra 	$L__BB0_14;

	mul.lo.s32 	%r232, %r18, -63;
	mad.lo.s32 	%r233, %r19, 24, %r18;
	mul.wide.s32 	%rd24, %r20, 3136;
	mul.lo.s32 	%r234, %r21, 56;
	cvt.s64.s32 	%rd25, %r234;
	cvt.s64.s32 	%rd26, %r22;
	add.s64 	%rd27, %rd24, %rd26;
	add.s64 	%rd28, %rd27, %rd25;
	shl.b64 	%rd29, %rd28, 6;
	cvt.s64.s32 	%rd30, %r232;
	add.s64 	%rd31, %rd29, %rd30;
	shl.b64 	%rd32, %rd31, 1;
	add.s64 	%rd23, %rd9, %rd32;
	shl.b32 	%r235, %r233, 1;
	mov.u32 	%r236, smem;
	add.s32 	%r237, %r236, %r235;
	add.s32 	%r231, %r237, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r231], [%rd23], 16;
	// end inline asm

$L__BB0_14:
	add.s32 	%r238, %r17, 32;
	shr.u32 	%r239, %r238, 31;
	add.s32 	%r240, %r238, %r239;
	shr.s32 	%r23, %r240, 1;
	shl.b32 	%r241, %r238, 3;
	shr.s32 	%r242, %r241, 31;
	shr.u32 	%r243, %r242, 28;
	add.s32 	%r244, %r241, %r243;
	and.b32  	%r245, %r244, -16;
	sub.s32 	%r24, %r241, %r245;
	add.s32 	%r25, %r23, %r2;
	setp.gt.s32 	%p14, %r25, 100351;
	@%p14 bra 	$L__BB0_17;

	mad.lo.s32 	%r246, %r25, -3135, %r24;
	add.s32 	%r26, %r246, -1;
	shl.b32 	%r247, %r24, 1;
	neg.s32 	%r248, %r247;
	mad.lo.s32 	%r249, %r25, 172425, %r248;
	add.s32 	%r27, %r249, -1;
	setp.gt.u32 	%p15, %r26, 55;
	setp.gt.u32 	%p16, %r27, 55;
	or.pred  	%p17, %p16, %p15;
	@%p17 bra 	$L__BB0_17;

	mul.lo.s32 	%r251, %r24, -63;
	mad.lo.s32 	%r252, %r23, 24, %r24;
	mul.wide.s32 	%rd34, %r25, 3136;
	mul.lo.s32 	%r253, %r26, 56;
	cvt.s64.s32 	%rd35, %r253;
	cvt.s64.s32 	%rd36, %r27;
	add.s64 	%rd37, %rd34, %rd36;
	add.s64 	%rd38, %rd37, %rd35;
	shl.b64 	%rd39, %rd38, 6;
	cvt.s64.s32 	%rd40, %r251;
	add.s64 	%rd41, %rd39, %rd40;
	shl.b64 	%rd42, %rd41, 1;
	add.s64 	%rd33, %rd9, %rd42;
	shl.b32 	%r254, %r252, 1;
	mov.u32 	%r255, smem;
	add.s32 	%r256, %r255, %r254;
	add.s32 	%r250, %r256, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r250], [%rd33], 16;
	// end inline asm

$L__BB0_17:
	add.s32 	%r257, %r17, 64;
	shr.u32 	%r258, %r257, 31;
	add.s32 	%r259, %r257, %r258;
	shr.s32 	%r28, %r259, 1;
	shl.b32 	%r260, %r257, 3;
	shr.s32 	%r261, %r260, 31;
	shr.u32 	%r262, %r261, 28;
	add.s32 	%r263, %r260, %r262;
	and.b32  	%r264, %r263, -16;
	sub.s32 	%r29, %r260, %r264;
	add.s32 	%r30, %r28, %r2;
	setp.gt.s32 	%p18, %r30, 100351;
	@%p18 bra 	$L__BB0_20;

	mad.lo.s32 	%r265, %r30, -3135, %r29;
	add.s32 	%r31, %r265, -1;
	shl.b32 	%r266, %r29, 1;
	neg.s32 	%r267, %r266;
	mad.lo.s32 	%r268, %r30, 172425, %r267;
	add.s32 	%r32, %r268, -1;
	setp.gt.u32 	%p19, %r31, 55;
	setp.gt.u32 	%p20, %r32, 55;
	or.pred  	%p21, %p20, %p19;
	@%p21 bra 	$L__BB0_20;

	mul.lo.s32 	%r270, %r29, -63;
	mad.lo.s32 	%r271, %r28, 24, %r29;
	mul.wide.s32 	%rd44, %r30, 3136;
	mul.lo.s32 	%r272, %r31, 56;
	cvt.s64.s32 	%rd45, %r272;
	cvt.s64.s32 	%rd46, %r32;
	add.s64 	%rd47, %rd44, %rd46;
	add.s64 	%rd48, %rd47, %rd45;
	shl.b64 	%rd49, %rd48, 6;
	cvt.s64.s32 	%rd50, %r270;
	add.s64 	%rd51, %rd49, %rd50;
	shl.b64 	%rd52, %rd51, 1;
	add.s64 	%rd43, %rd9, %rd52;
	shl.b32 	%r273, %r271, 1;
	mov.u32 	%r274, smem;
	add.s32 	%r275, %r274, %r273;
	add.s32 	%r269, %r275, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r269], [%rd43], 16;
	// end inline asm

$L__BB0_20:
	add.s32 	%r276, %r17, 96;
	shr.u32 	%r277, %r276, 31;
	add.s32 	%r278, %r276, %r277;
	shr.s32 	%r33, %r278, 1;
	shl.b32 	%r279, %r276, 3;
	shr.s32 	%r280, %r279, 31;
	shr.u32 	%r281, %r280, 28;
	add.s32 	%r282, %r279, %r281;
	and.b32  	%r283, %r282, -16;
	sub.s32 	%r34, %r279, %r283;
	add.s32 	%r35, %r33, %r2;
	setp.gt.s32 	%p22, %r35, 100351;
	@%p22 bra 	$L__BB0_23;

	mad.lo.s32 	%r284, %r35, -3135, %r34;
	add.s32 	%r36, %r284, -1;
	shl.b32 	%r285, %r34, 1;
	neg.s32 	%r286, %r285;
	mad.lo.s32 	%r287, %r35, 172425, %r286;
	add.s32 	%r37, %r287, -1;
	setp.gt.u32 	%p23, %r36, 55;
	setp.gt.u32 	%p24, %r37, 55;
	or.pred  	%p25, %p24, %p23;
	@%p25 bra 	$L__BB0_23;

	mul.lo.s32 	%r289, %r34, -63;
	mad.lo.s32 	%r290, %r33, 24, %r34;
	mul.wide.s32 	%rd54, %r35, 3136;
	mul.lo.s32 	%r291, %r36, 56;
	cvt.s64.s32 	%rd55, %r291;
	cvt.s64.s32 	%rd56, %r37;
	add.s64 	%rd57, %rd54, %rd56;
	add.s64 	%rd58, %rd57, %rd55;
	shl.b64 	%rd59, %rd58, 6;
	cvt.s64.s32 	%rd60, %r289;
	add.s64 	%rd61, %rd59, %rd60;
	shl.b64 	%rd62, %rd61, 1;
	add.s64 	%rd53, %rd9, %rd62;
	shl.b32 	%r292, %r290, 1;
	mov.u32 	%r293, smem;
	add.s32 	%r294, %r293, %r292;
	add.s32 	%r288, %r294, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r288], [%rd53], 16;
	// end inline asm

$L__BB0_23:
	add.s32 	%r827, %r17, 128;
	setp.lt.s32 	%p26, %r17, 0;
	@%p26 bra 	$L__BB0_11;

$L__BB0_24:
	setp.gt.s32 	%p27, %r1, 63;
	@%p27 bra 	$L__BB0_40;

	max.s32 	%r295, %r1, 32;
	add.s32 	%r296, %r295, 31;
	sub.s32 	%r39, %r296, %r1;
	shr.u32 	%r297, %r39, 5;
	add.s32 	%r298, %r297, 1;
	and.b32  	%r830, %r298, 3;
	setp.eq.s32 	%p28, %r830, 0;
	mov.u32 	%r831, %r1;
	@%p28 bra 	$L__BB0_30;

	mov.u32 	%r831, %r1;

$L__BB0_27:
	.pragma "nounroll";
	shl.b32 	%r299, %r831, 3;
	shr.s32 	%r300, %r299, 31;
	shr.u32 	%r301, %r300, 27;
	add.s32 	%r302, %r299, %r301;
	and.b32  	%r303, %r302, -32;
	sub.s32 	%r43, %r299, %r303;
	add.s32 	%r44, %r43, %r3;
	setp.gt.s32 	%p29, %r44, 63;
	@%p29 bra 	$L__BB0_29;

	shr.s32 	%r305, %r831, 31;
	shr.u32 	%r306, %r305, 30;
	add.s32 	%r307, %r831, %r306;
	shr.s32 	%r308, %r307, 2;
	mad.lo.s32 	%r309, %r308, 40, %r43;
	mul.wide.s32 	%rd64, %r308, 64;
	cvt.s64.s32 	%rd65, %r44;
	add.s64 	%rd66, %rd64, %rd65;
	shl.b64 	%rd67, %rd66, 1;
	add.s64 	%rd63, %rd10, %rd67;
	shl.b32 	%r310, %r309, 1;
	mov.u32 	%r311, smem;
	add.s32 	%r312, %r311, %r310;
	add.s32 	%r304, %r312, 6272;
	// begin inline asm
	cp.async.ca.shared.global [%r304], [%rd63], 16;
	// end inline asm

$L__BB0_29:
	add.s32 	%r831, %r831, 32;
	add.s32 	%r830, %r830, -1;
	setp.ne.s32 	%p30, %r830, 0;
	@%p30 bra 	$L__BB0_27;

$L__BB0_30:
	setp.lt.u32 	%p31, %r39, 96;
	@%p31 bra 	$L__BB0_40;

$L__BB0_31:
	.pragma "nounroll";
	shl.b32 	%r313, %r831, 3;
	shr.s32 	%r314, %r313, 31;
	shr.u32 	%r315, %r314, 27;
	add.s32 	%r316, %r313, %r315;
	and.b32  	%r317, %r316, -32;
	sub.s32 	%r49, %r313, %r317;
	add.s32 	%r50, %r49, %r3;
	setp.gt.s32 	%p32, %r50, 63;
	@%p32 bra 	$L__BB0_33;

	shr.s32 	%r319, %r831, 31;
	shr.u32 	%r320, %r319, 30;
	add.s32 	%r321, %r831, %r320;
	shr.s32 	%r322, %r321, 2;
	mad.lo.s32 	%r323, %r322, 40, %r49;
	mul.wide.s32 	%rd69, %r322, 64;
	cvt.s64.s32 	%rd70, %r50;
	add.s64 	%rd71, %rd69, %rd70;
	shl.b64 	%rd72, %rd71, 1;
	add.s64 	%rd68, %rd10, %rd72;
	shl.b32 	%r324, %r323, 1;
	mov.u32 	%r325, smem;
	add.s32 	%r326, %r325, %r324;
	add.s32 	%r318, %r326, 6272;
	// begin inline asm
	cp.async.ca.shared.global [%r318], [%rd68], 16;
	// end inline asm

$L__BB0_33:
	add.s32 	%r51, %r831, 32;
	shl.b32 	%r327, %r51, 3;
	shr.s32 	%r328, %r327, 31;
	shr.u32 	%r329, %r328, 27;
	add.s32 	%r330, %r327, %r329;
	and.b32  	%r331, %r330, -32;
	sub.s32 	%r52, %r327, %r331;
	add.s32 	%r53, %r52, %r3;
	setp.gt.s32 	%p33, %r53, 63;
	@%p33 bra 	$L__BB0_35;

	shr.s32 	%r333, %r51, 31;
	shr.u32 	%r334, %r333, 30;
	add.s32 	%r335, %r51, %r334;
	shr.s32 	%r336, %r335, 2;
	mad.lo.s32 	%r337, %r336, 40, %r52;
	mul.wide.s32 	%rd74, %r336, 64;
	cvt.s64.s32 	%rd75, %r53;
	add.s64 	%rd76, %rd74, %rd75;
	shl.b64 	%rd77, %rd76, 1;
	add.s64 	%rd73, %rd10, %rd77;
	shl.b32 	%r338, %r337, 1;
	mov.u32 	%r339, smem;
	add.s32 	%r340, %r339, %r338;
	add.s32 	%r332, %r340, 6272;
	// begin inline asm
	cp.async.ca.shared.global [%r332], [%rd73], 16;
	// end inline asm

$L__BB0_35:
	add.s32 	%r54, %r831, 64;
	shl.b32 	%r341, %r54, 3;
	shr.s32 	%r342, %r341, 31;
	shr.u32 	%r343, %r342, 27;
	add.s32 	%r344, %r341, %r343;
	and.b32  	%r345, %r344, -32;
	sub.s32 	%r55, %r341, %r345;
	add.s32 	%r56, %r55, %r3;
	setp.gt.s32 	%p34, %r56, 63;
	@%p34 bra 	$L__BB0_37;

	shr.s32 	%r347, %r54, 31;
	shr.u32 	%r348, %r347, 30;
	add.s32 	%r349, %r54, %r348;
	shr.s32 	%r350, %r349, 2;
	mad.lo.s32 	%r351, %r350, 40, %r55;
	mul.wide.s32 	%rd79, %r350, 64;
	cvt.s64.s32 	%rd80, %r56;
	add.s64 	%rd81, %rd79, %rd80;
	shl.b64 	%rd82, %rd81, 1;
	add.s64 	%rd78, %rd10, %rd82;
	shl.b32 	%r352, %r351, 1;
	mov.u32 	%r353, smem;
	add.s32 	%r354, %r353, %r352;
	add.s32 	%r346, %r354, 6272;
	// begin inline asm
	cp.async.ca.shared.global [%r346], [%rd78], 16;
	// end inline asm

$L__BB0_37:
	add.s32 	%r57, %r831, 96;
	shl.b32 	%r355, %r57, 3;
	shr.s32 	%r356, %r355, 31;
	shr.u32 	%r357, %r356, 27;
	add.s32 	%r358, %r355, %r357;
	and.b32  	%r359, %r358, -32;
	sub.s32 	%r58, %r355, %r359;
	add.s32 	%r59, %r58, %r3;
	setp.gt.s32 	%p35, %r59, 63;
	@%p35 bra 	$L__BB0_39;

	shr.s32 	%r361, %r57, 31;
	shr.u32 	%r362, %r361, 30;
	add.s32 	%r363, %r57, %r362;
	shr.s32 	%r364, %r363, 2;
	mad.lo.s32 	%r365, %r364, 40, %r58;
	mul.wide.s32 	%rd84, %r364, 64;
	cvt.s64.s32 	%rd85, %r59;
	add.s64 	%rd86, %rd84, %rd85;
	shl.b64 	%rd87, %rd86, 1;
	add.s64 	%rd83, %rd10, %rd87;
	shl.b32 	%r366, %r365, 1;
	mov.u32 	%r367, smem;
	add.s32 	%r368, %r367, %r366;
	add.s32 	%r360, %r368, 6272;
	// begin inline asm
	cp.async.ca.shared.global [%r360], [%rd83], 16;
	// end inline asm

$L__BB0_39:
	add.s32 	%r60, %r831, 128;
	setp.lt.s32 	%p36, %r831, -64;
	mov.u32 	%r831, %r60;
	@%p36 bra 	$L__BB0_31;

$L__BB0_40:
	// begin inline asm
	cp.async.commit_group;
	// end inline asm
	// begin inline asm
	cp.async.wait_group 0;
	// end inline asm

$L__BB0_41:
	bar.sync 	0;
	shl.b32 	%r370, %r1, 3;
	shr.s32 	%r371, %r370, 31;
	shr.u32 	%r372, %r371, 28;
	add.s32 	%r373, %r370, %r372;
	and.b32  	%r374, %r373, -16;
	sub.s32 	%r61, %r370, %r374;
	shr.u32 	%r375, %r1, 31;
	add.s32 	%r376, %r1, %r375;
	shr.s32 	%r377, %r376, 1;
	add.s32 	%r62, %r377, %r2;
	shr.u32 	%r378, %r371, 27;
	add.s32 	%r379, %r370, %r378;
	and.b32  	%r380, %r379, -32;
	sub.s32 	%r63, %r370, %r380;
	mad.lo.s32 	%r64, %r377, 24, %r61;
	mul.wide.s32 	%rd6, %r62, 3136;
	add.s32 	%r381, %r1, 32;
	shr.u32 	%r382, %r381, 31;
	add.s32 	%r383, %r381, %r382;
	shr.s32 	%r384, %r383, 1;
	shl.b32 	%r385, %r381, 3;
	shr.s32 	%r386, %r385, 31;
	shr.u32 	%r387, %r386, 28;
	add.s32 	%r388, %r385, %r387;
	and.b32  	%r389, %r388, -16;
	sub.s32 	%r65, %r385, %r389;
	add.s32 	%r66, %r384, %r2;
	shr.u32 	%r391, %r195, 30;
	add.s32 	%r392, %r1, %r391;
	shr.s32 	%r67, %r392, 2;
	shr.s32 	%r393, %r381, 31;
	shr.u32 	%r394, %r393, 30;
	add.s32 	%r395, %r381, %r394;
	shr.s32 	%r68, %r395, 2;
	shr.u32 	%r396, %r386, 27;
	add.s32 	%r397, %r385, %r396;
	and.b32  	%r398, %r397, -32;
	sub.s32 	%r69, %r385, %r398;
	mad.lo.s32 	%r70, %r384, 24, %r65;
	mul.wide.s32 	%rd7, %r66, 3136;
	add.s32 	%r399, %r1, 64;
	shr.u32 	%r400, %r399, 31;
	add.s32 	%r401, %r399, %r400;
	shr.s32 	%r402, %r401, 1;
	shl.b32 	%r403, %r399, 3;
	shr.s32 	%r404, %r403, 31;
	shr.u32 	%r405, %r404, 28;
	add.s32 	%r406, %r403, %r405;
	and.b32  	%r407, %r406, -16;
	sub.s32 	%r71, %r403, %r407;
	add.s32 	%r72, %r402, %r2;
	shr.s32 	%r408, %r399, 31;
	shr.u32 	%r409, %r408, 30;
	add.s32 	%r410, %r399, %r409;
	shr.s32 	%r73, %r410, 2;
	shr.u32 	%r411, %r404, 27;
	add.s32 	%r412, %r403, %r411;
	and.b32  	%r413, %r412, -32;
	sub.s32 	%r74, %r403, %r413;
	mad.lo.s32 	%r75, %r402, 24, %r71;
	mul.wide.s32 	%rd8, %r72, 3136;
	mov.u32 	%r833, 0;
	mov.f32 	%f86, 0f00000000;
	shl.b32 	%r463, %r64, 1;
	shl.b32 	%r481, %r70, 1;
	shl.b32 	%r501, %r75, 1;
	mov.f32 	%f87, %f86;
	mov.f32 	%f88, %f86;
	mov.f32 	%f89, %f86;
	mov.f32 	%f90, %f86;
	mov.f32 	%f91, %f86;
	mov.f32 	%f92, %f86;
	mov.f32 	%f93, %f86;
	mov.f32 	%f94, %f86;
	mov.f32 	%f95, %f86;
	mov.f32 	%f96, %f86;
	mov.f32 	%f97, %f86;
	mov.f32 	%f98, %f86;
	mov.f32 	%f99, %f86;
	mov.f32 	%f100, %f86;
	mov.f32 	%f101, %f86;

$L__BB0_42:
	mov.u32 	%r414, %tid.x;
	setp.lt.s32 	%p37, %r414, 32;
	@%p37 bra 	$L__BB0_44;
	bra.uni 	$L__BB0_43;

$L__BB0_44:
	add.s32 	%r77, %r833, 1;
	setp.lt.u32 	%p38, %r77, 36;
	@%p38 bra 	$L__BB0_46;
	bra.uni 	$L__BB0_45;

$L__BB0_46:
	mov.u32 	%r449, %tid.x;
	setp.gt.s32 	%p39, %r449, 127;
	shl.b32 	%r78, %r77, 4;
	@%p39 bra 	$L__BB0_74;

	mov.u32 	%r834, %tid.x;
	max.s32 	%r450, %r834, 96;
	add.s32 	%r451, %r450, 31;
	sub.s32 	%r452, %r451, %r834;
	shr.u32 	%r453, %r452, 5;
	add.s32 	%r454, %r453, 1;
	and.b32  	%r455, %r454, 3;
	setp.eq.s32 	%p40, %r455, 0;
	@%p40 bra 	$L__BB0_59;

	setp.gt.s32 	%p41, %r62, 100351;
	add.s32 	%r80, %r61, %r78;
	setp.gt.s32 	%p42, %r80, 575;
	or.pred  	%p43, %p41, %p42;
	@%p43 bra 	$L__BB0_51;

	add.s32 	%r750, %r61, %r78;
	mad.lo.s32 	%r456, %r62, -3135, -1;
	add.s32 	%r81, %r456, %r750;
	mad.lo.s32 	%r457, %r62, 175560, %r456;
	mad.lo.s32 	%r82, %r750, -2, %r457;
	setp.gt.u32 	%p44, %r81, 55;
	setp.gt.u32 	%p45, %r82, 55;
	or.pred  	%p46, %p45, %p44;
	@%p46 bra 	$L__BB0_51;

	add.s32 	%r759, %r833, 1;
	add.s32 	%r751, %r61, %r78;
	and.b32  	%r460, %r759, 1;
	mov.u32 	%r461, smem;
	mad.lo.s32 	%r462, %r460, 3072, %r461;
	add.s32 	%r464, %r462, %r463;
	mul.lo.s32 	%r465, %r81, 56;
	cvt.s64.s32 	%rd92, %r465;
	cvt.s64.s32 	%rd93, %r82;
	add.s64 	%rd94, %rd6, %rd93;
	add.s64 	%rd95, %rd94, %rd92;
	shl.b64 	%rd96, %rd95, 6;
	mul.lo.s32 	%r466, %r751, -63;
	cvt.s64.s32 	%rd97, %r466;
	add.s64 	%rd98, %rd96, %rd97;
	shl.b64 	%rd99, %rd98, 1;
	add.s64 	%rd91, %rd9, %rd99;
	add.s32 	%r458, %r464, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r458], [%rd91], 16;
	// end inline asm

$L__BB0_51:
	mov.u32 	%r467, %tid.x;
	max.s32 	%r468, %r467, 96;
	add.s32 	%r469, %r468, 31;
	sub.s32 	%r470, %r469, %r467;
	shr.u32 	%r471, %r470, 5;
	add.s32 	%r472, %r471, 1;
	and.b32  	%r473, %r472, 3;
	setp.eq.s32 	%p47, %r473, 1;
	add.s32 	%r834, %r467, 32;
	@%p47 bra 	$L__BB0_59;

	setp.gt.s32 	%p48, %r66, 100351;
	add.s32 	%r84, %r65, %r78;
	setp.gt.s32 	%p49, %r84, 575;
	or.pred  	%p50, %p48, %p49;
	@%p50 bra 	$L__BB0_55;

	add.s32 	%r762, %r65, %r78;
	mad.lo.s32 	%r474, %r66, -3135, -1;
	add.s32 	%r85, %r474, %r762;
	mad.lo.s32 	%r475, %r66, 175560, %r474;
	mad.lo.s32 	%r86, %r762, -2, %r475;
	setp.gt.u32 	%p51, %r85, 55;
	setp.gt.u32 	%p52, %r86, 55;
	or.pred  	%p53, %p52, %p51;
	@%p53 bra 	$L__BB0_55;

	add.s32 	%r763, %r65, %r78;
	add.s32 	%r753, %r833, 1;
	and.b32  	%r478, %r753, 1;
	mov.u32 	%r479, smem;
	mad.lo.s32 	%r480, %r478, 3072, %r479;
	add.s32 	%r482, %r480, %r481;
	mul.lo.s32 	%r483, %r85, 56;
	cvt.s64.s32 	%rd101, %r483;
	cvt.s64.s32 	%rd102, %r86;
	add.s64 	%rd103, %rd7, %rd102;
	add.s64 	%rd104, %rd103, %rd101;
	shl.b64 	%rd105, %rd104, 6;
	mul.lo.s32 	%r484, %r763, -63;
	cvt.s64.s32 	%rd106, %r484;
	add.s64 	%rd107, %rd105, %rd106;
	shl.b64 	%rd108, %rd107, 1;
	add.s64 	%rd100, %rd9, %rd108;
	add.s32 	%r476, %r482, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r476], [%rd100], 16;
	// end inline asm

$L__BB0_55:
	mov.u32 	%r770, %tid.x;
	max.s32 	%r769, %r770, 96;
	add.s32 	%r768, %r769, 31;
	sub.s32 	%r767, %r768, %r770;
	shr.u32 	%r766, %r767, 5;
	add.s32 	%r765, %r766, 1;
	and.b32  	%r764, %r765, 3;
	mov.u32 	%r760, %tid.x;
	setp.eq.s32 	%p54, %r764, 2;
	add.s32 	%r834, %r760, 64;
	@%p54 bra 	$L__BB0_59;

	mov.u32 	%r761, %tid.x;
	setp.gt.s32 	%p55, %r72, 100351;
	add.s32 	%r88, %r71, %r78;
	setp.gt.s32 	%p56, %r88, 575;
	add.s32 	%r834, %r761, 96;
	or.pred  	%p57, %p55, %p56;
	@%p57 bra 	$L__BB0_59;

	add.s32 	%r771, %r71, %r78;
	mad.lo.s32 	%r493, %r72, -3135, -1;
	add.s32 	%r90, %r493, %r771;
	mad.lo.s32 	%r494, %r72, 175560, %r493;
	mad.lo.s32 	%r91, %r771, -2, %r494;
	setp.gt.u32 	%p58, %r90, 55;
	setp.gt.u32 	%p59, %r91, 55;
	or.pred  	%p60, %p59, %p58;
	@%p60 bra 	$L__BB0_59;

	mov.u32 	%r774, %tid.x;
	add.s32 	%r834, %r774, 96;
	add.s32 	%r772, %r71, %r78;
	add.s32 	%r752, %r833, 1;
	and.b32  	%r498, %r752, 1;
	mov.u32 	%r499, smem;
	mad.lo.s32 	%r500, %r498, 3072, %r499;
	add.s32 	%r502, %r500, %r501;
	mul.lo.s32 	%r503, %r90, 56;
	cvt.s64.s32 	%rd110, %r503;
	cvt.s64.s32 	%rd111, %r91;
	add.s64 	%rd112, %rd8, %rd111;
	add.s64 	%rd113, %rd112, %rd110;
	shl.b64 	%rd114, %rd113, 6;
	mul.lo.s32 	%r504, %r772, -63;
	cvt.s64.s32 	%rd115, %r504;
	add.s64 	%rd116, %rd114, %rd115;
	shl.b64 	%rd117, %rd116, 1;
	add.s64 	%rd109, %rd9, %rd117;
	add.s32 	%r496, %r502, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r496], [%rd109], 16;
	// end inline asm

$L__BB0_59:
	setp.lt.u32 	%p61, %r452, 96;
	@%p61 bra 	$L__BB0_74;

	shl.b32 	%r835, %r834, 3;

$L__BB0_61:
	.pragma "nounroll";
	mov.u32 	%r97, %r834;
	shr.u32 	%r510, %r97, 31;
	add.s32 	%r511, %r97, %r510;
	shr.s32 	%r98, %r511, 1;
	add.s32 	%r99, %r98, %r2;
	shr.s32 	%r512, %r835, 31;
	shr.u32 	%r513, %r512, 28;
	add.s32 	%r514, %r835, %r513;
	and.b32  	%r515, %r514, -16;
	sub.s32 	%r100, %r835, %r515;
	add.s32 	%r101, %r100, %r78;
	setp.gt.s32 	%p62, %r99, 100351;
	setp.gt.s32 	%p63, %r101, 575;
	or.pred  	%p64, %p62, %p63;
	@%p64 bra 	$L__BB0_64;

	mad.lo.s32 	%r516, %r99, -3135, -1;
	add.s32 	%r102, %r516, %r101;
	mad.lo.s32 	%r517, %r99, 175560, %r516;
	mad.lo.s32 	%r103, %r101, -2, %r517;
	setp.gt.u32 	%p65, %r102, 55;
	setp.gt.u32 	%p66, %r103, 55;
	or.pred  	%p67, %p66, %p65;
	@%p67 bra 	$L__BB0_64;

	shr.s32 	%r782, %r835, 31;
	shr.u32 	%r781, %r782, 28;
	add.s32 	%r780, %r835, %r781;
	and.b32  	%r779, %r780, -16;
	sub.s32 	%r778, %r835, %r779;
	shr.u32 	%r777, %r97, 31;
	add.s32 	%r776, %r97, %r777;
	shr.s32 	%r775, %r776, 1;
	add.s32 	%r757, %r833, 1;
	and.b32  	%r520, %r757, 1;
	mov.u32 	%r521, smem;
	mad.lo.s32 	%r522, %r520, 3072, %r521;
	mad.lo.s32 	%r523, %r775, 24, %r778;
	shl.b32 	%r524, %r523, 1;
	add.s32 	%r525, %r522, %r524;
	mul.wide.s32 	%rd119, %r99, 3136;
	mul.lo.s32 	%r526, %r102, 56;
	cvt.s64.s32 	%rd120, %r526;
	cvt.s64.s32 	%rd121, %r103;
	add.s64 	%rd122, %rd119, %rd121;
	add.s64 	%rd123, %rd122, %rd120;
	shl.b64 	%rd124, %rd123, 6;
	mul.lo.s32 	%r527, %r101, -63;
	cvt.s64.s32 	%rd125, %r527;
	add.s64 	%rd126, %rd124, %rd125;
	shl.b64 	%rd127, %rd126, 1;
	add.s64 	%rd118, %rd9, %rd127;
	add.s32 	%r518, %r525, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r518], [%rd118], 16;
	// end inline asm

$L__BB0_64:
	add.s32 	%r104, %r97, 32;
	shr.u32 	%r528, %r104, 31;
	add.s32 	%r529, %r104, %r528;
	shr.s32 	%r105, %r529, 1;
	add.s32 	%r530, %r835, 256;
	shr.s32 	%r531, %r530, 31;
	shr.u32 	%r532, %r531, 28;
	add.s32 	%r533, %r530, %r532;
	and.b32  	%r534, %r533, -16;
	sub.s32 	%r106, %r530, %r534;
	add.s32 	%r107, %r105, %r2;
	add.s32 	%r108, %r106, %r78;
	setp.gt.s32 	%p68, %r107, 100351;
	setp.gt.s32 	%p69, %r108, 575;
	or.pred  	%p70, %p68, %p69;
	@%p70 bra 	$L__BB0_67;

	mad.lo.s32 	%r535, %r107, -3135, -1;
	add.s32 	%r109, %r535, %r108;
	mad.lo.s32 	%r536, %r107, 175560, %r535;
	mad.lo.s32 	%r110, %r108, -2, %r536;
	setp.gt.u32 	%p71, %r109, 55;
	setp.gt.u32 	%p72, %r110, 55;
	or.pred  	%p73, %p72, %p71;
	@%p73 bra 	$L__BB0_67;

	add.s32 	%r792, %r835, 256;
	add.s32 	%r791, %r97, 32;
	shr.s32 	%r790, %r792, 31;
	shr.u32 	%r789, %r790, 28;
	add.s32 	%r788, %r792, %r789;
	and.b32  	%r787, %r788, -16;
	sub.s32 	%r786, %r792, %r787;
	shr.u32 	%r785, %r791, 31;
	add.s32 	%r784, %r791, %r785;
	shr.s32 	%r783, %r784, 1;
	add.s32 	%r756, %r833, 1;
	and.b32  	%r539, %r756, 1;
	mov.u32 	%r540, smem;
	mad.lo.s32 	%r541, %r539, 3072, %r540;
	mad.lo.s32 	%r542, %r783, 24, %r786;
	shl.b32 	%r543, %r542, 1;
	add.s32 	%r544, %r541, %r543;
	mul.wide.s32 	%rd129, %r107, 3136;
	mul.lo.s32 	%r545, %r109, 56;
	cvt.s64.s32 	%rd130, %r545;
	cvt.s64.s32 	%rd131, %r110;
	add.s64 	%rd132, %rd129, %rd131;
	add.s64 	%rd133, %rd132, %rd130;
	shl.b64 	%rd134, %rd133, 6;
	mul.lo.s32 	%r546, %r108, -63;
	cvt.s64.s32 	%rd135, %r546;
	add.s64 	%rd136, %rd134, %rd135;
	shl.b64 	%rd137, %rd136, 1;
	add.s64 	%rd128, %rd9, %rd137;
	add.s32 	%r537, %r544, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r537], [%rd128], 16;
	// end inline asm

$L__BB0_67:
	add.s32 	%r111, %r104, 32;
	shr.u32 	%r547, %r111, 31;
	add.s32 	%r548, %r111, %r547;
	shr.s32 	%r112, %r548, 1;
	add.s32 	%r549, %r835, 512;
	shr.s32 	%r550, %r549, 31;
	shr.u32 	%r551, %r550, 28;
	add.s32 	%r552, %r549, %r551;
	and.b32  	%r553, %r552, -16;
	sub.s32 	%r113, %r549, %r553;
	add.s32 	%r114, %r112, %r2;
	add.s32 	%r115, %r113, %r78;
	setp.gt.s32 	%p74, %r114, 100351;
	setp.gt.s32 	%p75, %r115, 575;
	or.pred  	%p76, %p74, %p75;
	@%p76 bra 	$L__BB0_70;

	mad.lo.s32 	%r554, %r114, -3135, -1;
	add.s32 	%r116, %r554, %r115;
	mad.lo.s32 	%r555, %r114, 175560, %r554;
	mad.lo.s32 	%r117, %r115, -2, %r555;
	setp.gt.u32 	%p77, %r116, 55;
	setp.gt.u32 	%p78, %r117, 55;
	or.pred  	%p79, %p78, %p77;
	@%p79 bra 	$L__BB0_70;

	add.s32 	%r803, %r97, 32;
	add.s32 	%r802, %r835, 512;
	add.s32 	%r801, %r803, 32;
	shr.s32 	%r800, %r802, 31;
	shr.u32 	%r799, %r800, 28;
	add.s32 	%r798, %r802, %r799;
	and.b32  	%r797, %r798, -16;
	sub.s32 	%r796, %r802, %r797;
	shr.u32 	%r795, %r801, 31;
	add.s32 	%r794, %r801, %r795;
	shr.s32 	%r793, %r794, 1;
	add.s32 	%r755, %r833, 1;
	and.b32  	%r558, %r755, 1;
	mov.u32 	%r559, smem;
	mad.lo.s32 	%r560, %r558, 3072, %r559;
	mad.lo.s32 	%r561, %r793, 24, %r796;
	shl.b32 	%r562, %r561, 1;
	add.s32 	%r563, %r560, %r562;
	mul.wide.s32 	%rd139, %r114, 3136;
	mul.lo.s32 	%r564, %r116, 56;
	cvt.s64.s32 	%rd140, %r564;
	cvt.s64.s32 	%rd141, %r117;
	add.s64 	%rd142, %rd139, %rd141;
	add.s64 	%rd143, %rd142, %rd140;
	shl.b64 	%rd144, %rd143, 6;
	mul.lo.s32 	%r565, %r115, -63;
	cvt.s64.s32 	%rd145, %r565;
	add.s64 	%rd146, %rd144, %rd145;
	shl.b64 	%rd147, %rd146, 1;
	add.s64 	%rd138, %rd9, %rd147;
	add.s32 	%r556, %r563, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r556], [%rd138], 16;
	// end inline asm

$L__BB0_70:
	add.s32 	%r566, %r111, 32;
	shr.u32 	%r567, %r566, 31;
	add.s32 	%r568, %r566, %r567;
	shr.s32 	%r118, %r568, 1;
	add.s32 	%r569, %r835, 768;
	shr.s32 	%r570, %r569, 31;
	shr.u32 	%r571, %r570, 28;
	add.s32 	%r572, %r569, %r571;
	and.b32  	%r573, %r572, -16;
	sub.s32 	%r119, %r569, %r573;
	add.s32 	%r120, %r118, %r2;
	add.s32 	%r121, %r119, %r78;
	setp.gt.s32 	%p80, %r120, 100351;
	setp.gt.s32 	%p81, %r121, 575;
	or.pred  	%p82, %p80, %p81;
	@%p82 bra 	$L__BB0_73;

	mad.lo.s32 	%r574, %r120, -3135, -1;
	add.s32 	%r122, %r574, %r121;
	mad.lo.s32 	%r575, %r120, 175560, %r574;
	mad.lo.s32 	%r123, %r121, -2, %r575;
	setp.gt.u32 	%p83, %r122, 55;
	setp.gt.u32 	%p84, %r123, 55;
	or.pred  	%p85, %p84, %p83;
	@%p85 bra 	$L__BB0_73;

	add.s32 	%r815, %r97, 32;
	add.s32 	%r814, %r815, 32;
	add.s32 	%r813, %r835, 768;
	add.s32 	%r812, %r814, 32;
	shr.s32 	%r811, %r813, 31;
	shr.u32 	%r810, %r811, 28;
	add.s32 	%r809, %r813, %r810;
	and.b32  	%r808, %r809, -16;
	sub.s32 	%r807, %r813, %r808;
	shr.u32 	%r806, %r812, 31;
	add.s32 	%r805, %r812, %r806;
	shr.s32 	%r804, %r805, 1;
	add.s32 	%r754, %r833, 1;
	and.b32  	%r578, %r754, 1;
	mov.u32 	%r579, smem;
	mad.lo.s32 	%r580, %r578, 3072, %r579;
	mad.lo.s32 	%r581, %r804, 24, %r807;
	shl.b32 	%r582, %r581, 1;
	add.s32 	%r583, %r580, %r582;
	mul.wide.s32 	%rd149, %r120, 3136;
	mul.lo.s32 	%r584, %r122, 56;
	cvt.s64.s32 	%rd150, %r584;
	cvt.s64.s32 	%rd151, %r123;
	add.s64 	%rd152, %rd149, %rd151;
	add.s64 	%rd153, %rd152, %rd150;
	shl.b64 	%rd154, %rd153, 6;
	mul.lo.s32 	%r585, %r121, -63;
	cvt.s64.s32 	%rd155, %r585;
	add.s64 	%rd156, %rd154, %rd155;
	shl.b64 	%rd157, %rd156, 1;
	add.s64 	%rd148, %rd9, %rd157;
	add.s32 	%r576, %r583, 128;
	// begin inline asm
	cp.async.ca.shared.global [%r576], [%rd148], 16;
	// end inline asm

$L__BB0_73:
	add.s32 	%r835, %r835, 1024;
	add.s32 	%r834, %r97, 128;
	setp.lt.s32 	%p86, %r97, 0;
	@%p86 bra 	$L__BB0_61;

$L__BB0_74:
	mov.u32 	%r747, %tid.x;
	setp.gt.s32 	%p87, %r747, 63;
	@%p87 bra 	$L__BB0_94;

	add.s32 	%r758, %r833, 1;
	mov.u32 	%r837, %tid.x;
	max.s32 	%r587, %r837, 32;
	add.s32 	%r588, %r587, 31;
	sub.s32 	%r589, %r588, %r837;
	shr.u32 	%r590, %r589, 5;
	add.s32 	%r591, %r590, 1;
	and.b32  	%r592, %r591, 3;
	setp.eq.s32 	%p88, %r592, 0;
	and.b32  	%r594, %r758, 1;
	mov.u32 	%r595, smem;
	mad.lo.s32 	%r596, %r594, 1280, %r595;
	add.s32 	%r127, %r596, 6272;
	@%p88 bra 	$L__BB0_84;

	add.s32 	%r599, %r63, %r3;
	setp.gt.s32 	%p89, %r599, 63;
	add.s32 	%r128, %r67, %r78;
	setp.gt.s32 	%p90, %r128, 575;
	or.pred  	%p91, %p90, %p89;
	@%p91 bra 	$L__BB0_78;

	mul.wide.s32 	%rd159, %r128, 64;
	cvt.s64.s32 	%rd160, %r599;
	add.s64 	%rd161, %rd159, %rd160;
	shl.b64 	%rd162, %rd161, 1;
	add.s64 	%rd158, %rd10, %rd162;
	mad.lo.s32 	%r604, %r67, 40, %r63;
	shl.b32 	%r605, %r604, 1;
	add.s32 	%r600, %r127, %r605;
	// begin inline asm
	cp.async.ca.shared.global [%r600], [%rd158], 16;
	// end inline asm

$L__BB0_78:
	mov.u32 	%r606, %tid.x;
	max.s32 	%r607, %r606, 32;
	add.s32 	%r608, %r607, 31;
	sub.s32 	%r609, %r608, %r606;
	shr.u32 	%r610, %r609, 5;
	add.s32 	%r611, %r610, 1;
	and.b32  	%r612, %r611, 3;
	setp.eq.s32 	%p92, %r612, 1;
	add.s32 	%r837, %r606, 32;
	@%p92 bra 	$L__BB0_84;

	add.s32 	%r615, %r69, %r3;
	setp.gt.s32 	%p93, %r615, 63;
	add.s32 	%r130, %r68, %r78;
	setp.gt.s32 	%p94, %r130, 575;
	or.pred  	%p95, %p94, %p93;
	@%p95 bra 	$L__BB0_81;

	mul.wide.s32 	%rd164, %r130, 64;
	cvt.s64.s32 	%rd165, %r615;
	add.s64 	%rd166, %rd164, %rd165;
	shl.b64 	%rd167, %rd166, 1;
	add.s64 	%rd163, %rd10, %rd167;
	mad.lo.s32 	%r620, %r68, 40, %r69;
	shl.b32 	%r621, %r620, 1;
	add.s32 	%r616, %r127, %r621;
	// begin inline asm
	cp.async.ca.shared.global [%r616], [%rd163], 16;
	// end inline asm

$L__BB0_81:
	mov.u32 	%r823, %tid.x;
	mov.u32 	%r822, %tid.x;
	max.s32 	%r821, %r822, 32;
	add.s32 	%r820, %r821, 31;
	sub.s32 	%r819, %r820, %r822;
	shr.u32 	%r818, %r819, 5;
	add.s32 	%r817, %r818, 1;
	and.b32  	%r816, %r817, 3;
	setp.eq.s32 	%p96, %r816, 2;
	add.s32 	%r837, %r822, 64;
	@%p96 bra 	$L__BB0_84;

	mov.u32 	%r824, %tid.x;
	add.s32 	%r631, %r74, %r3;
	setp.gt.s32 	%p97, %r631, 63;
	add.s32 	%r132, %r73, %r78;
	setp.gt.s32 	%p98, %r132, 575;
	add.s32 	%r837, %r824, 96;
	or.pred  	%p99, %p98, %p97;
	@%p99 bra 	$L__BB0_84;

	mul.wide.s32 	%rd169, %r132, 64;
	cvt.s64.s32 	%rd170, %r631;
	add.s64 	%rd171, %rd169, %rd170;
	shl.b64 	%rd172, %rd171, 1;
	add.s64 	%rd168, %rd10, %rd172;
	mad.lo.s32 	%r637, %r73, 40, %r74;
	shl.b32 	%r638, %r637, 1;
	add.s32 	%r633, %r127, %r638;
	// begin inline asm
	cp.async.ca.shared.global [%r633], [%rd168], 16;
	// end inline asm

$L__BB0_84:
	setp.lt.u32 	%p100, %r589, 96;
	@%p100 bra 	$L__BB0_94;

$L__BB0_85:
	.pragma "nounroll";
	shl.b32 	%r645, %r837, 3;
	shr.s32 	%r646, %r645, 31;
	shr.u32 	%r647, %r646, 27;
	add.s32 	%r648, %r645, %r647;
	and.b32  	%r649, %r648, -32;
	sub.s32 	%r138, %r645, %r649;
	shr.s32 	%r650, %r837, 31;
	shr.u32 	%r651, %r650, 30;
	add.s32 	%r652, %r837, %r651;
	shr.s32 	%r139, %r652, 2;
	add.s32 	%r140, %r139, %r78;
	add.s32 	%r141, %r138, %r3;
	setp.gt.s32 	%p101, %r140, 575;
	setp.gt.s32 	%p102, %r141, 63;
	or.pred  	%p103, %p101, %p102;
	@%p103 bra 	$L__BB0_87;

	mad.lo.s32 	%r654, %r139, 40, %r138;
	mul.wide.s32 	%rd174, %r140, 64;
	cvt.s64.s32 	%rd175, %r141;
	add.s64 	%rd176, %rd174, %rd175;
	shl.b64 	%rd177, %rd176, 1;
	add.s64 	%rd173, %rd10, %rd177;
	shl.b32 	%r655, %r654, 1;
	add.s32 	%r653, %r127, %r655;
	// begin inline asm
	cp.async.ca.shared.global [%r653], [%rd173], 16;
	// end inline asm

$L__BB0_87:
	add.s32 	%r656, %r837, 32;
	shr.s32 	%r657, %r656, 31;
	shr.u32 	%r658, %r657, 30;
	add.s32 	%r659, %r656, %r658;
	shr.s32 	%r142, %r659, 2;
	shl.b32 	%r660, %r656, 3;
	shr.s32 	%r661, %r660, 31;
	shr.u32 	%r662, %r661, 27;
	add.s32 	%r663, %r660, %r662;
	and.b32  	%r664, %r663, -32;
	sub.s32 	%r143, %r660, %r664;
	add.s32 	%r144, %r142, %r78;
	add.s32 	%r145, %r143, %r3;
	setp.gt.s32 	%p104, %r144, 575;
	setp.gt.s32 	%p105, %r145, 63;
	or.pred  	%p106, %p104, %p105;
	@%p106 bra 	$L__BB0_89;

	mad.lo.s32 	%r666, %r142, 40, %r143;
	mul.wide.s32 	%rd179, %r144, 64;
	cvt.s64.s32 	%rd180, %r145;
	add.s64 	%rd181, %rd179, %rd180;
	shl.b64 	%rd182, %rd181, 1;
	add.s64 	%rd178, %rd10, %rd182;
	shl.b32 	%r667, %r666, 1;
	add.s32 	%r665, %r127, %r667;
	// begin inline asm
	cp.async.ca.shared.global [%r665], [%rd178], 16;
	// end inline asm

$L__BB0_89:
	add.s32 	%r668, %r837, 64;
	shr.s32 	%r669, %r668, 31;
	shr.u32 	%r670, %r669, 30;
	add.s32 	%r671, %r668, %r670;
	shr.s32 	%r146, %r671, 2;
	shl.b32 	%r672, %r668, 3;
	shr.s32 	%r673, %r672, 31;
	shr.u32 	%r674, %r673, 27;
	add.s32 	%r675, %r672, %r674;
	and.b32  	%r676, %r675, -32;
	sub.s32 	%r147, %r672, %r676;
	add.s32 	%r148, %r146, %r78;
	add.s32 	%r149, %r147, %r3;
	setp.gt.s32 	%p107, %r148, 575;
	setp.gt.s32 	%p108, %r149, 63;
	or.pred  	%p109, %p107, %p108;
	@%p109 bra 	$L__BB0_91;

	mad.lo.s32 	%r678, %r146, 40, %r147;
	mul.wide.s32 	%rd184, %r148, 64;
	cvt.s64.s32 	%rd185, %r149;
	add.s64 	%rd186, %rd184, %rd185;
	shl.b64 	%rd187, %rd186, 1;
	add.s64 	%rd183, %rd10, %rd187;
	shl.b32 	%r679, %r678, 1;
	add.s32 	%r677, %r127, %r679;
	// begin inline asm
	cp.async.ca.shared.global [%r677], [%rd183], 16;
	// end inline asm

$L__BB0_91:
	add.s32 	%r680, %r837, 96;
	shr.s32 	%r681, %r680, 31;
	shr.u32 	%r682, %r681, 30;
	add.s32 	%r683, %r680, %r682;
	shr.s32 	%r150, %r683, 2;
	shl.b32 	%r684, %r680, 3;
	shr.s32 	%r685, %r684, 31;
	shr.u32 	%r686, %r685, 27;
	add.s32 	%r687, %r684, %r686;
	and.b32  	%r688, %r687, -32;
	sub.s32 	%r151, %r684, %r688;
	add.s32 	%r152, %r150, %r78;
	add.s32 	%r153, %r151, %r3;
	setp.gt.s32 	%p110, %r152, 575;
	setp.gt.s32 	%p111, %r153, 63;
	or.pred  	%p112, %p110, %p111;
	@%p112 bra 	$L__BB0_93;

	mad.lo.s32 	%r690, %r150, 40, %r151;
	mul.wide.s32 	%rd189, %r152, 64;
	cvt.s64.s32 	%rd190, %r153;
	add.s64 	%rd191, %rd189, %rd190;
	shl.b64 	%rd192, %rd191, 1;
	add.s64 	%rd188, %rd10, %rd192;
	shl.b32 	%r691, %r690, 1;
	add.s32 	%r689, %r127, %r691;
	// begin inline asm
	cp.async.ca.shared.global [%r689], [%rd188], 16;
	// end inline asm

$L__BB0_93:
	add.s32 	%r154, %r837, 128;
	setp.lt.s32 	%p113, %r837, -64;
	mov.u32 	%r837, %r154;
	@%p113 bra 	$L__BB0_85;

$L__BB0_94:
	// begin inline asm
	cp.async.commit_group;
	// end inline asm
	// begin inline asm
	cp.async.wait_group 0;
	// end inline asm
	bra.uni 	$L__BB0_95;

$L__BB0_43:
	and.b32  	%r415, %r833, 1;
	mov.u32 	%r416, smem;
	mad.lo.s32 	%r417, %r415, 1280, %r416;
	add.s32 	%r418, %r417, 6272;
	mad.lo.s32 	%r419, %r415, 3072, %r416;
	add.s32 	%r420, %r419, 128;
	mov.u32 	%r421, 40;
	wmma.load.b.sync.aligned.row.m16n16k16.shared.f16 	{%r422, %r423, %r424, %r425, %r426, %r427, %r428, %r429}, [%r418], %r421;
	add.s32 	%r430, %r417, 6304;
	wmma.load.b.sync.aligned.row.m16n16k16.shared.f16 	{%r431, %r432, %r433, %r434, %r435, %r436, %r437, %r438}, [%r430], %r421;
	mad.lo.s32 	%r439, %r4, 768, %r420;
	mov.u32 	%r440, 24;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r441, %r442, %r443, %r444, %r445, %r446, %r447, %r448}, [%r439], %r440;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f86, %f87, %f88, %f89, %f90, %f91, %f92, %f93}, {%r441, %r442, %r443, %r444, %r445, %r446, %r447, %r448}, {%r422, %r423, %r424, %r425, %r426, %r427, %r428, %r429}, {%f86, %f87, %f88, %f89, %f90, %f91, %f92, %f93};
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f94, %f95, %f96, %f97, %f98, %f99, %f100, %f101}, {%r441, %r442, %r443, %r444, %r445, %r446, %r447, %r448}, {%r431, %r432, %r433, %r434, %r435, %r436, %r437, %r438}, {%f94, %f95, %f96, %f97, %f98, %f99, %f100, %f101};
	bra.uni 	$L__BB0_95;

$L__BB0_45:
	// begin inline asm
	cp.async.wait_group 0;
	// end inline asm

$L__BB0_95:
	bar.sync 	0;
	add.s32 	%r833, %r833, 1;
	setp.lt.u32 	%p114, %r833, 36;
	@%p114 bra 	$L__BB0_42;

	mov.u32 	%r748, %tid.x;
	setp.lt.s32 	%p136, %r748, 32;
	@%p136 bra 	$L__BB0_114;

	mov.u32 	%r745, %tid.x;
	shl.b32 	%r693, %r4, 11;
	mov.u32 	%r694, smem;
	add.s32 	%r695, %r694, %r693;
	mov.u32 	%r696, 32;
	wmma.store.d.sync.aligned.row.m16n16k16.shared.f32 	[%r695], {%f86, %f87, %f88, %f89, %f90, %f91, %f92, %f93}, %r696;
	add.s32 	%r697, %r695, 64;
	wmma.store.d.sync.aligned.row.m16n16k16.shared.f32 	[%r697], {%f94, %f95, %f96, %f97, %f98, %f99, %f100, %f101}, %r696;
	bar.sync 	0;
	setp.gt.s32 	%p116, %r745, 2047;
	@%p116 bra 	$L__BB0_114;

	mov.u32 	%r842, %tid.x;
	max.s32 	%r699, %r842, 1920;
	add.s32 	%r700, %r699, 127;
	sub.s32 	%r157, %r700, %r842;
	shr.u32 	%r701, %r157, 7;
	add.s32 	%r702, %r701, 1;
	and.b32  	%r841, %r702, 3;
	setp.eq.s32 	%p117, %r841, 0;
	@%p117 bra 	$L__BB0_103;

	ld.param.u64 	%rd225, [conv2d_implicit_gemm_param_2];
	mov.u32 	%r842, %tid.x;
	shl.b32 	%r703, %r842, 2;
	add.s32 	%r839, %r694, %r703;
	cvta.to.global.u64 	%rd198, %rd225;

$L__BB0_100:
	.pragma "nounroll";
	shr.s32 	%r706, %r842, 31;
	shr.u32 	%r707, %r706, 27;
	add.s32 	%r708, %r842, %r707;
	shr.s32 	%r709, %r708, 5;
	add.s32 	%r165, %r709, %r2;
	and.b32  	%r710, %r708, -32;
	sub.s32 	%r711, %r842, %r710;
	add.s32 	%r166, %r711, %r3;
	setp.gt.s32 	%p118, %r165, 100351;
	setp.gt.s32 	%p119, %r166, 63;
	or.pred  	%p120, %p118, %p119;
	@%p120 bra 	$L__BB0_102;

	ld.shared.f32 	%f65, [%r839];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f65;}

	// end inline asm
	mul.wide.s32 	%rd195, %r165, 64;
	cvt.s64.s32 	%rd196, %r166;
	add.s64 	%rd197, %rd195, %rd196;
	shl.b64 	%rd199, %rd197, 1;
	add.s64 	%rd200, %rd198, %rd199;
	st.global.u16 	[%rd200], %rs1;

$L__BB0_102:
	add.s32 	%r842, %r842, 128;
	add.s32 	%r839, %r839, 512;
	add.s32 	%r841, %r841, -1;
	setp.ne.s32 	%p121, %r841, 0;
	@%p121 bra 	$L__BB0_100;

$L__BB0_103:
	setp.lt.u32 	%p122, %r157, 384;
	@%p122 bra 	$L__BB0_114;

	ld.param.u64 	%rd226, [conv2d_implicit_gemm_param_2];
	shl.b32 	%r712, %r842, 2;
	add.s32 	%r714, %r694, %r712;
	add.s32 	%r843, %r714, 1024;
	cvta.to.global.u64 	%rd204, %rd226;

$L__BB0_105:
	.pragma "nounroll";
	shr.s32 	%r716, %r842, 31;
	shr.u32 	%r717, %r716, 27;
	add.s32 	%r718, %r842, %r717;
	shr.s32 	%r719, %r718, 5;
	add.s32 	%r175, %r719, %r2;
	and.b32  	%r720, %r718, -32;
	sub.s32 	%r721, %r842, %r720;
	add.s32 	%r176, %r721, %r3;
	setp.gt.s32 	%p123, %r175, 100351;
	setp.gt.s32 	%p124, %r176, 63;
	or.pred  	%p125, %p123, %p124;
	@%p125 bra 	$L__BB0_107;

	add.s32 	%r741, %r843, -1024;
	ld.shared.f32 	%f66, [%r741];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f66;}

	// end inline asm
	mul.wide.s32 	%rd201, %r175, 64;
	cvt.s64.s32 	%rd202, %r176;
	add.s64 	%rd203, %rd201, %rd202;
	shl.b64 	%rd205, %rd203, 1;
	add.s64 	%rd206, %rd204, %rd205;
	st.global.u16 	[%rd206], %rs2;

$L__BB0_107:
	add.s32 	%r178, %r842, 128;
	shr.s32 	%r722, %r178, 31;
	shr.u32 	%r723, %r722, 27;
	add.s32 	%r724, %r178, %r723;
	shr.s32 	%r725, %r724, 5;
	and.b32  	%r726, %r724, -32;
	sub.s32 	%r727, %r178, %r726;
	add.s32 	%r179, %r725, %r2;
	add.s32 	%r180, %r727, %r3;
	setp.gt.s32 	%p126, %r179, 100351;
	setp.gt.s32 	%p127, %r180, 63;
	or.pred  	%p128, %p126, %p127;
	@%p128 bra 	$L__BB0_109;

	add.s32 	%r742, %r843, -1024;
	ld.shared.f32 	%f67, [%r742+512];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f67;}

	// end inline asm
	mul.wide.s32 	%rd207, %r179, 64;
	cvt.s64.s32 	%rd208, %r180;
	add.s64 	%rd209, %rd207, %rd208;
	shl.b64 	%rd211, %rd209, 1;
	add.s64 	%rd212, %rd204, %rd211;
	st.global.u16 	[%rd212], %rs3;

$L__BB0_109:
	add.s32 	%r181, %r178, 128;
	shr.s32 	%r728, %r181, 31;
	shr.u32 	%r729, %r728, 27;
	add.s32 	%r730, %r181, %r729;
	shr.s32 	%r731, %r730, 5;
	and.b32  	%r732, %r730, -32;
	sub.s32 	%r733, %r181, %r732;
	add.s32 	%r182, %r731, %r2;
	add.s32 	%r183, %r733, %r3;
	setp.gt.s32 	%p129, %r182, 100351;
	setp.gt.s32 	%p130, %r183, 63;
	or.pred  	%p131, %p129, %p130;
	@%p131 bra 	$L__BB0_111;

	add.s32 	%r743, %r843, -1024;
	ld.shared.f32 	%f68, [%r743+1024];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f68;}

	// end inline asm
	mul.wide.s32 	%rd213, %r182, 64;
	cvt.s64.s32 	%rd214, %r183;
	add.s64 	%rd215, %rd213, %rd214;
	shl.b64 	%rd217, %rd215, 1;
	add.s64 	%rd218, %rd204, %rd217;
	st.global.u16 	[%rd218], %rs4;

$L__BB0_111:
	add.s32 	%r734, %r181, 128;
	shr.s32 	%r735, %r734, 31;
	shr.u32 	%r736, %r735, 27;
	add.s32 	%r737, %r734, %r736;
	shr.s32 	%r738, %r737, 5;
	and.b32  	%r739, %r737, -32;
	sub.s32 	%r740, %r734, %r739;
	add.s32 	%r184, %r738, %r2;
	add.s32 	%r185, %r740, %r3;
	setp.gt.s32 	%p132, %r184, 100351;
	setp.gt.s32 	%p133, %r185, 63;
	or.pred  	%p134, %p132, %p133;
	@%p134 bra 	$L__BB0_113;

	add.s32 	%r744, %r843, -1024;
	ld.shared.f32 	%f69, [%r744+1536];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f69;}

	// end inline asm
	mul.wide.s32 	%rd219, %r184, 64;
	cvt.s64.s32 	%rd220, %r185;
	add.s64 	%rd221, %rd219, %rd220;
	shl.b64 	%rd223, %rd221, 1;
	add.s64 	%rd224, %rd204, %rd223;
	st.global.u16 	[%rd224], %rs5;

$L__BB0_113:
	add.s32 	%r843, %r843, 2048;
	add.s32 	%r187, %r842, 512;
	setp.lt.s32 	%p135, %r842, 1536;
	mov.u32 	%r842, %r187;
	@%p135 bra 	$L__BB0_105;

$L__BB0_114:
	ret;

}

